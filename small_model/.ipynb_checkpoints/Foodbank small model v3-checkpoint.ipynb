{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0aa7ae9-f9cd-492f-8f87-5e0c63ef709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import copy\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7bde29-0d3e-4cc2-b0a4-1f1dcdb63807",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # specify ImageNet mean and standard deviation\n",
    "    \"IMG_MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"IMG_STD\": [0.229, 0.224, 0.225],\n",
    "    \"INIT_LR\": 1e-4,\n",
    "    \"NUM_EPOCHS\": 200,\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    # specify the loss weights\n",
    "    \"LABELS_PRDTYPE\": 1.0,\n",
    "    \"LABELS_WEIGHT\": 1.0,\n",
    "    \"LABELS_HALAL\": 1.0,\n",
    "    \"LABELS_HEALTHY\": 1.0,\n",
    "    \"MODEL_PATH\": os.path.sep.join([\"output\", \"detector.pth\"]),\n",
    "    \"LE_PATH_PRDTYPE\": os.path.sep.join([\"output\", \"le_prdtype.pickle\"]),\n",
    "    \"LE_PATH_WEIGHT\": os.path.sep.join([\"output\", \"le_weight.pickle\"]),\n",
    "    \"LE_PATH_HALAL\": os.path.sep.join([\"output\", \"le_halal.pickle\"]),\n",
    "    \"LE_PATH_HEALTHY\": os.path.sep.join([\"output\", \"le_healthy.pickle\"]),\n",
    "    \"PIN_MEMORY\": True if torch.cuda.is_available() else False,\n",
    "    \"DATA_BASE_PATH\": os.path.join('..', 'rshiny', 'www', 'all_images'),\n",
    "    \"NEW_DATA_BASE_PATH\": os.path.join('..', 'small_model', 'new_imgs'),\n",
    "    \"BASE_PATH\": os.path.join('..'),\n",
    "}\n",
    "\n",
    "# create output folder\n",
    "if not os.path.exists(\"output\"):\n",
    "    !mkdir -p {\"output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8415bd71-0c8f-4516-abce-bbcb6473de4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HalalStatus</th>\n",
       "      <th>HealthStatus</th>\n",
       "      <th>new_camera</th>\n",
       "      <th>tag</th>\n",
       "      <th>group</th>\n",
       "      <th>tokeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_20230428_123528_jpg.rf.5687b7b914f6d9aa98c...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_20230428_123522_jpg.rf.204ff37f497f2dce442...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_20230428_123708_jpg.rf.141ecd0cefaea75c0b7...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_20230428_123521_jpg.rf.1069b402272252862ec...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_20230428_123659_jpg.rf.5e1b6c4caabe48cf360...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0  IMG_20230428_123528_jpg.rf.5687b7b914f6d9aa98c...   \n",
       "1  IMG_20230428_123522_jpg.rf.204ff37f497f2dce442...   \n",
       "2  IMG_20230428_123708_jpg.rf.141ecd0cefaea75c0b7...   \n",
       "3  IMG_20230428_123521_jpg.rf.1069b402272252862ec...   \n",
       "4  IMG_20230428_123659_jpg.rf.5e1b6c4caabe48cf360...   \n",
       "\n",
       "                                label ProductType    Weight HalalStatus  \\\n",
       "0  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "1  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "2  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "3  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "4  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "\n",
       "  HealthStatus  new_camera  tag  group  tokeep  \n",
       "0   NonHealthy           0  NaN    1.0    True  \n",
       "1   NonHealthy           0  NaN    1.0    True  \n",
       "2   NonHealthy           0  NaN    2.0   False  \n",
       "3   NonHealthy           0  NaN    1.0    True  \n",
       "4   NonHealthy           0  NaN    2.0   False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_group_df = pd.read_csv(os.path.join(CONFIGS['BASE_PATH'], \"bayesian_model/product_group.csv\"))\n",
    "# prd_group_df['group'] = pd.to_numeric(prd_group_df['group'], errors='coerce')\n",
    "prd_group_df['group'] = prd_group_df['group'].fillna(0)\n",
    "# prd_group_df['tokeep'] = (prd_group_df['group'].astype(int) >= 1) & (prd_group_df['group'].astype(int) <= 7)\n",
    "prd_group_df['tokeep'] = prd_group_df['group'].astype(int) == 1\n",
    "prd_group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fecab9-5ae1-4a6a-b81b-df92cfca1678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6850, 13)\n",
      "(662, 14)\n",
      "(10, 14)\n",
      "(672, 14)\n"
     ]
    }
   ],
   "source": [
    "annotations_0 = pd.read_csv(\"../master_list.csv\")\n",
    "print(annotations_0.shape)\n",
    "annotations_0 = pd.merge(annotations_0, prd_group_df[[\"filepath\", \"tokeep\"]], how='left', on='filepath')\n",
    "\n",
    "annotations_1 = annotations_0.loc[annotations_0[\"tokeep\"]==True]\n",
    "annotations_1 = annotations_1.groupby('ProductType').apply(lambda x: x.sample(n=min(len(x), 10))).reset_index(drop=True)\n",
    "print(annotations_1.shape)\n",
    "\n",
    "annotations_2 = annotations_0[annotations_0['tag'] == 'TestforMode5']\n",
    "annotations_2.reset_index(drop=True, inplace=True)\n",
    "print(annotations_2.shape)\n",
    "\n",
    "# Concatenate the two dataframes vertically\n",
    "annotations = pd.concat([annotations_1, annotations_2], ignore_index=True)\n",
    "annotations.reset_index(drop=True, inplace=True)\n",
    "print(annotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ad46a5-20c4-458c-bebd-c635fe1b3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6bb8c4-a6d0-473f-9921-c560544ee7c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'BeehoonVermicelli_500-599g_Halal_NonHealthy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'BeehoonVermicelli_500-599g_Halal_NonHealthy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m testLabels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_healthy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le_healthy\u001b[38;5;241m.\u001b[39mtransform(split_labels[\u001b[38;5;241m7\u001b[39m])\n\u001b[1;32m     67\u001b[0m trainLabels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_total\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le_total\u001b[38;5;241m.\u001b[39mfit_transform(split_labels[\u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m---> 68\u001b[0m testLabels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels_total\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mle_total\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Convert NumPy arrays to PyTorch tensors\u001b[39;00m\n\u001b[1;32m     72\u001b[0m trainImages, testImages \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(trainImages), torch\u001b[38;5;241m.\u001b[39mtensor(testImages)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'BeehoonVermicelli_500-599g_Halal_NonHealthy'"
     ]
    }
   ],
   "source": [
    "# Initialize lists for processed data\n",
    "data, imagePaths, filenames = [], [], []\n",
    "\n",
    "# Process each annotation entry\n",
    "for idx, row in annotations.iterrows():\n",
    "    filepath = row[\"filepath\"]\n",
    "    imagePath = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], filepath)\n",
    "    # if row['Type'] == 'old':\n",
    "    #     imagePath = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], filepath)\n",
    "    # else:\n",
    "    #     imagePath = os.path.join(\"/content\", CONFIGS[\"NEW_DATA_BASE_PATH\"], filepath)\n",
    "    image = cv2.imread(imagePath)\n",
    "    \n",
    "    # Preprocess image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (60, 60))\n",
    "\n",
    "    # Append processed data to lists\n",
    "    data.append(image)\n",
    "    imagePaths.append(imagePath)\n",
    "    # filenames.append(filepath.rsplit('.', 1)[0])\n",
    "    filenames.append(filepath)\n",
    "\n",
    "# Convert data to NumPy arrays for machine learning processing\n",
    "labels = {\n",
    "    'labels_prdtype': annotations['ProductType'],\n",
    "    'labels_weight': annotations['Weight'],\n",
    "    'labels_halal': annotations['HalalStatus'],\n",
    "    'labels_healthy': annotations['HealthStatus'],\n",
    "    'labels_total': annotations['label']\n",
    "}\n",
    "\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "for label_name, label_data in labels.items():\n",
    "    labels[label_name] = np.array(label_data)\n",
    "\n",
    "# Split the data and labels into training and testing sets\n",
    "split = train_test_split(data, *labels.values(), imagePaths, filenames,\n",
    "                         test_size=0.3, random_state=42, stratify=labels['labels_prdtype'])\n",
    "\n",
    "# Unpack the data split\n",
    "(trainImages, testImages, *split_labels, trainPaths, testPaths, trainFilenames, testFilenames) = split\n",
    "\n",
    "# Create label encoders and transform labels\n",
    "le_prdtype = LabelEncoder()\n",
    "le_weight = LabelEncoder()\n",
    "le_halal = LabelEncoder()\n",
    "le_healthy = LabelEncoder()\n",
    "le_total = LabelEncoder()\n",
    "\n",
    "trainLabels = {}\n",
    "testLabels = {}\n",
    "\n",
    "# Fit label encoders and transform labels\n",
    "trainLabels['labels_prdtype'] = le_prdtype.fit_transform(split_labels[0])\n",
    "testLabels['labels_prdtype'] = le_prdtype.transform(split_labels[1])\n",
    "\n",
    "trainLabels['labels_weight'] = le_weight.fit_transform(split_labels[2])\n",
    "testLabels['labels_weight'] = le_weight.transform(split_labels[3])\n",
    "\n",
    "trainLabels['labels_halal'] = le_halal.fit_transform(split_labels[4])\n",
    "testLabels['labels_halal'] = le_halal.transform(split_labels[5])\n",
    "\n",
    "trainLabels['labels_healthy'] = le_healthy.fit_transform(split_labels[6])\n",
    "testLabels['labels_healthy'] = le_healthy.transform(split_labels[7])\n",
    "\n",
    "trainLabels['labels_total'] = le_total.fit_transform(split_labels[8])\n",
    "testLabels['labels_total'] = le_total.transform(split_labels[9])\n",
    "\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "trainImages, testImages = torch.tensor(trainImages), torch.tensor(testImages)\n",
    "for label_name in labels.keys():\n",
    "    trainLabels[label_name] = torch.tensor(trainLabels[label_name])\n",
    "    testLabels[label_name] = torch.tensor(testLabels[label_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0617d-27a1-46d6-8eb1-b0de887a744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    # Initialize the constructor\n",
    "    def __init__(self, images, labels, filenames, transforms=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.filenames = filenames\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Grab the image, labels, and its bounding box coordinates\n",
    "        image = self.images[index]\n",
    "        label_prdtype = self.labels['labels_prdtype'][index]\n",
    "        label_weight = self.labels['labels_weight'][index]\n",
    "        label_halal = self.labels['labels_halal'][index]\n",
    "        label_healthy = self.labels['labels_healthy'][index]\n",
    "        filename = self.filenames[index]\n",
    "\n",
    "        # Transpose the image such that its channel dimension becomes the leading one\n",
    "        image = image.permute(2, 0, 1)\n",
    "\n",
    "        # Check to see if we have any image transformations to apply and if so, apply them\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Return a tuple of the images, labels, and bounding box coordinates\n",
    "        return (image, label_prdtype, label_weight, label_halal, label_healthy, filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.images)\n",
    "\n",
    "# Define normalization and augmentation transforms\n",
    "normalization_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIGS['IMG_MEAN'], std=CONFIGS['IMG_STD'])\n",
    "])\n",
    "\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(20),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n",
    "])\n",
    "\n",
    "# Combine augmentation and normalization for training\n",
    "train_transforms = transforms.Compose([augmentation_transforms, normalization_transforms])\n",
    "test_transforms = normalization_transforms\n",
    "\n",
    "# Create PyTorch datasets\n",
    "trainDS = CustomTensorDataset(trainImages, trainLabels, trainFilenames, transforms=train_transforms)\n",
    "testDS = CustomTensorDataset(testImages, testLabels, testFilenames, transforms=test_transforms)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"[INFO] total training samples: {}...\".format(len(trainDS)))\n",
    "print(\"[INFO] total test samples: {}...\".format(len(testDS)))\n",
    "\n",
    "# Calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDS) // CONFIGS['BATCH_SIZE']\n",
    "valSteps = len(testDS) // CONFIGS['BATCH_SIZE']\n",
    "\n",
    "# Create data loaders\n",
    "# trainLoader = DataLoader(trainDS, batch_size=CONFIGS['BATCH_SIZE'], shuffle=True,\n",
    "#                          num_workers=os.cpu_count(), pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "# testLoader = DataLoader(testDS, batch_size=CONFIGS['BATCH_SIZE'],\n",
    "#                         num_workers=os.cpu_count(), pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "\n",
    "trainLoader = DataLoader(trainDS, batch_size=CONFIGS['BATCH_SIZE'], shuffle=True, num_workers=0, pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "testLoader = DataLoader(testDS, batch_size=CONFIGS['BATCH_SIZE'], num_workers=0, pin_memory=CONFIGS['PIN_MEMORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7dce84-8039-456c-a0fc-1c9d3651dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MultiHeadResNet model\n",
    "class MultiHeadResNet(nn.Module):\n",
    "    def __init__(self, num_classes_prdtype, num_classes_weight, num_classes_halal, num_classes_healthy):\n",
    "        super(MultiHeadResNet, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "        # Define custom fully connected layers for each prediction head\n",
    "        self.fc_prdtype = nn.Linear(num_ftrs, num_classes_prdtype)\n",
    "        self.fc_weight = nn.Linear(num_ftrs, num_classes_weight)\n",
    "        self.fc_halal = nn.Linear(num_ftrs, num_classes_halal)\n",
    "        self.fc_healthy = nn.Linear(num_ftrs, num_classes_healthy)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        prdtype = self.fc_prdtype(x)\n",
    "        weight = self.fc_weight(x)\n",
    "        halal = self.fc_halal(x)\n",
    "        healthy = self.fc_healthy(x)\n",
    "        return prdtype, weight, halal, healthy\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    corrects = torch.sum(preds == labels.data)\n",
    "    return corrects.double() / labels.size(0)\n",
    "\n",
    "# Training and Validation Loop with Early Stopping\n",
    "def train_model(model, criteria, optimizer, train_loader, test_loader, device, num_epochs=25, early_stopping_patience=10):\n",
    "    criterion_prdtype, criterion_weight, criterion_halal, criterion_healthy = criteria\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc_prdtype': [],\n",
    "        'train_acc_weight': [],\n",
    "        'train_acc_halal': [],\n",
    "        'train_acc_healthy': [],\n",
    "        'train_acc_overall': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc_prdtype': [],\n",
    "        'val_acc_weight': [],\n",
    "        'val_acc_halal': [],\n",
    "        'val_acc_healthy': [],\n",
    "        'val_acc_overall': [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_prdtype = 0\n",
    "            running_corrects_weight = 0\n",
    "            running_corrects_halal = 0\n",
    "            running_corrects_healthy = 0\n",
    "            running_corrects_overall = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for inputs, label_prdtype, label_weight, label_halal, label_healthy, _ in train_loader if phase == 'train' else test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                label_prdtype = label_prdtype.to(device)\n",
    "                label_weight = label_weight.to(device)\n",
    "                label_halal = label_halal.to(device)\n",
    "                label_healthy = label_healthy.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_prdtype, outputs_weight, outputs_halal, outputs_healthy = model(inputs)\n",
    "                    loss_prdtype = criterion_prdtype(outputs_prdtype, label_prdtype)\n",
    "                    loss_weight = criterion_weight(outputs_weight, label_weight)\n",
    "                    loss_halal = criterion_halal(outputs_halal, label_halal)\n",
    "                    loss_healthy = criterion_healthy(outputs_healthy, label_healthy)\n",
    "                    loss = loss_prdtype + loss_weight + loss_halal + loss_healthy  # Total loss\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects_prdtype += calculate_accuracy(outputs_prdtype, label_prdtype) * inputs.size(0)\n",
    "                running_corrects_weight += calculate_accuracy(outputs_weight, label_weight) * inputs.size(0)\n",
    "                running_corrects_halal += calculate_accuracy(outputs_halal, label_halal) * inputs.size(0)\n",
    "                running_corrects_healthy += calculate_accuracy(outputs_healthy, label_healthy) * inputs.size(0)\n",
    "                correct_preds_overall = ((outputs_prdtype.argmax(1) == label_prdtype) &\n",
    "                                         (outputs_weight.argmax(1) == label_weight) &\n",
    "                                         (outputs_halal.argmax(1) == label_halal) &\n",
    "                                         (outputs_healthy.argmax(1) == label_healthy))\n",
    "                running_corrects_overall += correct_preds_overall.sum().item()\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_acc_prdtype = running_corrects_prdtype / total_samples\n",
    "            epoch_acc_weight = running_corrects_weight / total_samples\n",
    "            epoch_acc_halal = running_corrects_halal / total_samples\n",
    "            epoch_acc_healthy = running_corrects_healthy / total_samples\n",
    "            epoch_acc_overall = running_corrects_overall / total_samples\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc_overall))\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    print(\"new loss obtained\")\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f\"tmp epochs_no_improve: {epochs_no_improve}\")\n",
    "\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered at epoch: {}\".format(epoch + 1))\n",
    "                    model.load_state_dict(best_model_wts)\n",
    "                    return model, history\n",
    "            \n",
    "            print(f\"epochs_no_improve: {epochs_no_improve}\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "# Example usage of the function\n",
    "# Assuming CONFIGS, trainLoader, testLoader, etc. are already defined\n",
    "num_classes_prdtype = len(np.unique(trainLabels['labels_prdtype']))\n",
    "num_classes_weight = len(np.unique(trainLabels['labels_weight']))\n",
    "num_classes_halal = len(np.unique(trainLabels['labels_halal']))\n",
    "num_classes_healthy = len(np.unique(trainLabels['labels_healthy']))\n",
    "\n",
    "custom_resnet_model = MultiHeadResNet(num_classes_prdtype, num_classes_weight, num_classes_halal, num_classes_healthy)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_resnet_model = custom_resnet_model.to(device)\n",
    "\n",
    "criterion_prdtype = nn.CrossEntropyLoss()\n",
    "criterion_weight = nn.CrossEntropyLoss()\n",
    "criterion_halal = nn.CrossEntropyLoss()\n",
    "criterion_healthy = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(custom_resnet_model.parameters(), lr=CONFIGS['INIT_LR'])\n",
    "\n",
    "criteria = (criterion_prdtype, criterion_weight, criterion_halal, criterion_healthy)\n",
    "\n",
    "# Start time\n",
    "print(\"Model training started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model_ft, history = train_model(custom_resnet_model, criteria, optimizer, trainLoader, testLoader, device, num_epochs=CONFIGS['NUM_EPOCHS'])\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "print(\"Model training completed...\")\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time spent: {round(execution_time/60,2)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c51679-7fe1-4459-b105-9692bf06a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), 'output/multi_head_model.pth')\n",
    "\n",
    "print(\"[INFO] saving label encoder...\")\n",
    "f = open(CONFIGS[\"LE_PATH_PRDTYPE\"], \"wb\")\n",
    "f.write(pickle.dumps(le_prdtype))\n",
    "f.close()\n",
    "f = open(CONFIGS[\"LE_PATH_WEIGHT\"], \"wb\")\n",
    "f.write(pickle.dumps(le_weight))\n",
    "f.close()\n",
    "f = open(CONFIGS[\"LE_PATH_HALAL\"], \"wb\")\n",
    "f.write(pickle.dumps(le_halal))\n",
    "f.close()\n",
    "f = open(CONFIGS[\"LE_PATH_HEALTHY\"], \"wb\")\n",
    "f.write(pickle.dumps(le_healthy))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de520c9-41e2-4c11-bed9-67caf2609523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracies: {'ProductType': 0.9893617021276596, 'Weight': 0.9851063829787234, 'HalalStatus': 0.9744680851063829, 'HealthStatus': 0.9872340425531915, 'Total': 0.9468085106382979}\n",
      "Test Accuracies: {'ProductType': 0.6732673267326733, 'Weight': 0.5544554455445545, 'HalalStatus': 0.7524752475247525, 'HealthStatus': 0.8910891089108911, 'Total': 0.41089108910891087}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, dataset_size, num_mc_samples=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_counts = {'ProductType': 0, 'Weight': 0, 'HalalStatus': 0, 'HealthStatus': 0, 'Total': 0}\n",
    "    total_distances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, labels_prdtype, labels_weight, labels_halal, labels_healthy, filenames) in data_loader:\n",
    "            images = images.to(CONFIGS['DEVICE'])\n",
    "            labels_prdtype, labels_weight, labels_halal, labels_healthy = labels_prdtype.to(CONFIGS['DEVICE']), labels_weight.to(CONFIGS['DEVICE']), labels_halal.to(CONFIGS['DEVICE']), labels_healthy.to(CONFIGS['DEVICE'])\n",
    "\n",
    "            # Forward pass\n",
    "            out1, out2, out3, out4 = model(images)\n",
    "\n",
    "            # Store deterministic predictions\n",
    "            det_pred_prdtype = out1.argmax(1)\n",
    "            det_pred_weight = out2.argmax(1)\n",
    "            det_pred_halal = out3.argmax(1)\n",
    "            det_pred_healthy = out4.argmax(1)\n",
    "\n",
    "            # # Monte Carlo Dropout\n",
    "            # mc_distances = []\n",
    "            # model.train()  # Enable dropout\n",
    "            # for i in range(num_mc_samples):\n",
    "            #     mc_out1, mc_out2, mc_out3, mc_out4, _ = model(images)\n",
    "            #     mc_pred_prdtype = mc_out1.argmax(1)\n",
    "            #     mc_pred_weight = mc_out2.argmax(1)\n",
    "            #     mc_pred_halal = mc_out3.argmax(1)\n",
    "            #     mc_pred_healthy = mc_out4.argmax(1)\n",
    "\n",
    "            #     distance = (det_pred_prdtype != mc_pred_prdtype).float() + \\\n",
    "            #                (det_pred_weight != mc_pred_weight).float() + \\\n",
    "            #                (det_pred_halal != mc_pred_halal).float() + \\\n",
    "            #                (det_pred_healthy != mc_pred_healthy).float()\n",
    "\n",
    "            #     mc_distances.append(distance)\n",
    "\n",
    "            # # Calculate average distance\n",
    "            # avg_distance = torch.stack(mc_distances).mean(0)\n",
    "            # total_distances.extend(avg_distance.cpu().numpy().tolist())\n",
    "\n",
    "            # Restore to evaluation mode\n",
    "            # model.eval()\n",
    "\n",
    "            # Update correct counts for each category\n",
    "            correct_counts['ProductType'] += (out1.argmax(1) == labels_prdtype).float().sum().item()\n",
    "            correct_counts['Weight'] += (out2.argmax(1) == labels_weight).float().sum().item()\n",
    "            correct_counts['HalalStatus'] += (out3.argmax(1) == labels_halal).float().sum().item()\n",
    "            correct_counts['HealthStatus'] += (out4.argmax(1) == labels_healthy).float().sum().item()\n",
    "            correct_counts['Total'] += ((out1.argmax(1) == labels_prdtype) & (out2.argmax(1) == labels_weight) & (out3.argmax(1) == labels_halal) & (out4.argmax(1) == labels_healthy)).float().sum().item()\n",
    "\n",
    "    # Calculate accuracies\n",
    "    accuracies = {key: correct_counts[key] / dataset_size for key in correct_counts}\n",
    "    # avg_total_distance = sum(total_distances) / len(total_distances)\n",
    "\n",
    "    # Plot histogram of distances\n",
    "    # plt.hist(total_distances, bins=30, alpha=0.7, label='Total Distances', color='b')\n",
    "    # plt.xlabel('Distance')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.title(f'Distribution of Total Distances: N={len(total_distances)}')\n",
    "    # plt.show()\n",
    "\n",
    "    # return accuracies, avg_total_distance, total_distances\n",
    "    return accuracies\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracies = evaluate_model(model_ft, trainLoader, len(trainDS))\n",
    "print(f\"Training Accuracies: {train_accuracies}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracies= evaluate_model(model_ft, testLoader, len(testDS))\n",
    "print(f\"Test Accuracies: {test_accuracies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1911e938-4e3e-4d1c-b7a7-79ea6b2d7324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>CorrectTotalLabel</th>\n",
       "      <th>ProductType_AdultMilk</th>\n",
       "      <th>ProductType_BabyMilkPowder</th>\n",
       "      <th>ProductType_Babyfood</th>\n",
       "      <th>ProductType_BeehoonVermicelliMeesua</th>\n",
       "      <th>ProductType_BiscuitsCrackersCookies</th>\n",
       "      <th>ProductType_Book</th>\n",
       "      <th>ProductType_BreakfastCerealsCornflakes</th>\n",
       "      <th>ProductType_CannedPacketCreamersSweet</th>\n",
       "      <th>...</th>\n",
       "      <th>Weight_400-499g</th>\n",
       "      <th>Weight_500-599g</th>\n",
       "      <th>Weight_600-699g</th>\n",
       "      <th>Weight_700-799g</th>\n",
       "      <th>Weight_800-899g</th>\n",
       "      <th>Weight_900-999g</th>\n",
       "      <th>HalalStatus_Halal</th>\n",
       "      <th>HalalStatus_NonHalal</th>\n",
       "      <th>HealthStatus_Healthy</th>\n",
       "      <th>HealthStatus_NonHealthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_3533_jpeg.rf.7c479d2b82aa319692d4c74ba4acf...</td>\n",
       "      <td>Nuts_300-399g_NonHalal_NonHealthy</td>\n",
       "      <td>-2.493247</td>\n",
       "      <td>-2.458110</td>\n",
       "      <td>-0.752643</td>\n",
       "      <td>3.418245</td>\n",
       "      <td>1.410686</td>\n",
       "      <td>-1.484503</td>\n",
       "      <td>-3.675712</td>\n",
       "      <td>-4.696049</td>\n",
       "      <td>...</td>\n",
       "      <td>1.202736</td>\n",
       "      <td>-0.929754</td>\n",
       "      <td>-2.064444</td>\n",
       "      <td>-3.176098</td>\n",
       "      <td>-1.962588</td>\n",
       "      <td>-3.577293</td>\n",
       "      <td>-3.286956</td>\n",
       "      <td>2.287906</td>\n",
       "      <td>-4.930899</td>\n",
       "      <td>2.953754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_2282_JPG.rf.55ff8559732da3aeccad420b17c3f9...</td>\n",
       "      <td>BeehoonVermicelliMeesua_200-299g_Halal_NonHealthy</td>\n",
       "      <td>0.131873</td>\n",
       "      <td>0.044445</td>\n",
       "      <td>-0.519548</td>\n",
       "      <td>7.003169</td>\n",
       "      <td>-0.729551</td>\n",
       "      <td>-3.285197</td>\n",
       "      <td>-1.949823</td>\n",
       "      <td>-2.659935</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359658</td>\n",
       "      <td>0.070565</td>\n",
       "      <td>-1.163762</td>\n",
       "      <td>-2.505035</td>\n",
       "      <td>-1.926098</td>\n",
       "      <td>-2.406361</td>\n",
       "      <td>3.796422</td>\n",
       "      <td>-2.279831</td>\n",
       "      <td>-1.371966</td>\n",
       "      <td>1.770485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_20230428_123200_jpg.rf.34cc17cc4cb5a707327...</td>\n",
       "      <td>Sugar_800-899g_NonHalal_NonHealthy</td>\n",
       "      <td>-2.153643</td>\n",
       "      <td>2.140464</td>\n",
       "      <td>-0.861953</td>\n",
       "      <td>-2.464264</td>\n",
       "      <td>0.675603</td>\n",
       "      <td>-1.984505</td>\n",
       "      <td>-1.661865</td>\n",
       "      <td>-4.919771</td>\n",
       "      <td>...</td>\n",
       "      <td>1.070148</td>\n",
       "      <td>-1.785139</td>\n",
       "      <td>-3.493709</td>\n",
       "      <td>-2.736918</td>\n",
       "      <td>6.794210</td>\n",
       "      <td>-0.917141</td>\n",
       "      <td>-3.683269</td>\n",
       "      <td>4.121268</td>\n",
       "      <td>-4.709025</td>\n",
       "      <td>1.865833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crackers_200-299g_0311_Halal_25_png.rf.c55545c...</td>\n",
       "      <td>BiscuitsCrackersCookies_200-299g_Halal_NonHealthy</td>\n",
       "      <td>-1.230329</td>\n",
       "      <td>-2.523535</td>\n",
       "      <td>0.557129</td>\n",
       "      <td>-2.749804</td>\n",
       "      <td>8.671530</td>\n",
       "      <td>-4.183312</td>\n",
       "      <td>-3.959238</td>\n",
       "      <td>-3.233744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550248</td>\n",
       "      <td>-2.701535</td>\n",
       "      <td>-0.062121</td>\n",
       "      <td>-1.760023</td>\n",
       "      <td>-0.467273</td>\n",
       "      <td>-4.762103</td>\n",
       "      <td>3.189244</td>\n",
       "      <td>-1.663041</td>\n",
       "      <td>-4.486520</td>\n",
       "      <td>1.161967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_6362_jpeg.rf.16a8b744b3be105dc728d6b6bfefc...</td>\n",
       "      <td>OtherBakingNeeds_900-999g_Halal_NonHealthy</td>\n",
       "      <td>-0.172884</td>\n",
       "      <td>1.589988</td>\n",
       "      <td>-0.664545</td>\n",
       "      <td>-1.997344</td>\n",
       "      <td>-0.707637</td>\n",
       "      <td>-3.679213</td>\n",
       "      <td>-1.114295</td>\n",
       "      <td>-2.144860</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.347105</td>\n",
       "      <td>-3.294689</td>\n",
       "      <td>-0.024379</td>\n",
       "      <td>-3.188744</td>\n",
       "      <td>1.041424</td>\n",
       "      <td>6.573578</td>\n",
       "      <td>2.014626</td>\n",
       "      <td>-1.763547</td>\n",
       "      <td>-4.737069</td>\n",
       "      <td>4.561938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Filename  \\\n",
       "0  IMG_3533_jpeg.rf.7c479d2b82aa319692d4c74ba4acf...   \n",
       "1  IMG_2282_JPG.rf.55ff8559732da3aeccad420b17c3f9...   \n",
       "2  IMG_20230428_123200_jpg.rf.34cc17cc4cb5a707327...   \n",
       "3  Crackers_200-299g_0311_Halal_25_png.rf.c55545c...   \n",
       "4  IMG_6362_jpeg.rf.16a8b744b3be105dc728d6b6bfefc...   \n",
       "\n",
       "                                   CorrectTotalLabel  ProductType_AdultMilk  \\\n",
       "0                  Nuts_300-399g_NonHalal_NonHealthy              -2.493247   \n",
       "1  BeehoonVermicelliMeesua_200-299g_Halal_NonHealthy               0.131873   \n",
       "2                 Sugar_800-899g_NonHalal_NonHealthy              -2.153643   \n",
       "3  BiscuitsCrackersCookies_200-299g_Halal_NonHealthy              -1.230329   \n",
       "4         OtherBakingNeeds_900-999g_Halal_NonHealthy              -0.172884   \n",
       "\n",
       "   ProductType_BabyMilkPowder  ProductType_Babyfood  \\\n",
       "0                   -2.458110             -0.752643   \n",
       "1                    0.044445             -0.519548   \n",
       "2                    2.140464             -0.861953   \n",
       "3                   -2.523535              0.557129   \n",
       "4                    1.589988             -0.664545   \n",
       "\n",
       "   ProductType_BeehoonVermicelliMeesua  ProductType_BiscuitsCrackersCookies  \\\n",
       "0                             3.418245                             1.410686   \n",
       "1                             7.003169                            -0.729551   \n",
       "2                            -2.464264                             0.675603   \n",
       "3                            -2.749804                             8.671530   \n",
       "4                            -1.997344                            -0.707637   \n",
       "\n",
       "   ProductType_Book  ProductType_BreakfastCerealsCornflakes  \\\n",
       "0         -1.484503                               -3.675712   \n",
       "1         -3.285197                               -1.949823   \n",
       "2         -1.984505                               -1.661865   \n",
       "3         -4.183312                               -3.959238   \n",
       "4         -3.679213                               -1.114295   \n",
       "\n",
       "   ProductType_CannedPacketCreamersSweet  ...  Weight_400-499g  \\\n",
       "0                              -4.696049  ...         1.202736   \n",
       "1                              -2.659935  ...        -0.359658   \n",
       "2                              -4.919771  ...         1.070148   \n",
       "3                              -3.233744  ...         0.550248   \n",
       "4                              -2.144860  ...        -1.347105   \n",
       "\n",
       "   Weight_500-599g  Weight_600-699g  Weight_700-799g  Weight_800-899g  \\\n",
       "0        -0.929754        -2.064444        -3.176098        -1.962588   \n",
       "1         0.070565        -1.163762        -2.505035        -1.926098   \n",
       "2        -1.785139        -3.493709        -2.736918         6.794210   \n",
       "3        -2.701535        -0.062121        -1.760023        -0.467273   \n",
       "4        -3.294689        -0.024379        -3.188744         1.041424   \n",
       "\n",
       "   Weight_900-999g  HalalStatus_Halal  HalalStatus_NonHalal  \\\n",
       "0        -3.577293          -3.286956              2.287906   \n",
       "1        -2.406361           3.796422             -2.279831   \n",
       "2        -0.917141          -3.683269              4.121268   \n",
       "3        -4.762103           3.189244             -1.663041   \n",
       "4         6.573578           2.014626             -1.763547   \n",
       "\n",
       "   HealthStatus_Healthy  HealthStatus_NonHealthy  \n",
       "0             -4.930899                 2.953754  \n",
       "1             -1.371966                 1.770485  \n",
       "2             -4.709025                 1.865833  \n",
       "3             -4.486520                 1.161967  \n",
       "4             -4.737069                 4.561938  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model2(model, data_loader, le_prdtype, le_weight, le_halal, le_healthy):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, labels_prdtype, labels_weight, labels_halal, labels_healthy, filenames) in data_loader:\n",
    "            images = images.to(CONFIGS['DEVICE'])\n",
    "            out1, out2, out3, out4 = model(images)\n",
    "\n",
    "            for idx in range(len(filenames)):\n",
    "                correct_label = f\"{le_prdtype.classes_[labels_prdtype[idx]]}_{le_weight.classes_[labels_weight[idx]]}_{le_halal.classes_[labels_halal[idx]]}_{le_healthy.classes_[labels_healthy[idx]]}\"\n",
    "                row = [filenames[idx], correct_label]\n",
    "                row.extend(out1[idx].cpu().numpy())\n",
    "                row.extend(out2[idx].cpu().numpy())\n",
    "                row.extend(out3[idx].cpu().numpy())\n",
    "                row.extend(out4[idx].cpu().numpy())\n",
    "                results.append(row)\n",
    "\n",
    "    # Define column names\n",
    "    column_names = ['Filename', 'CorrectTotalLabel']\n",
    "    column_names += ['ProductType_' + name for name in le_prdtype.classes_]\n",
    "    column_names += ['Weight_' + name for name in le_weight.classes_]\n",
    "    column_names += ['HalalStatus_' + name for name in le_halal.classes_]\n",
    "    column_names += ['HealthStatus_' + name for name in le_healthy.classes_]\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=column_names)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Usage of the function\n",
    "train_results_df = evaluate_model2(model_ft, trainLoader, le_prdtype, le_weight, le_halal, le_healthy)\n",
    "test_results_df = evaluate_model2(model_ft, testLoader, le_prdtype, le_weight, le_halal, le_healthy)\n",
    "\n",
    "# Concatenate the training and test results\n",
    "combined_results_df = pd.concat([train_results_df, test_results_df], axis=0)\n",
    "combined_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the combined results\n",
    "print(\"Combined Results:\")\n",
    "combined_results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9c7f644-2873-424e-a5d0-a5449bc59145",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results_df.to_csv('new_imgs_results_small_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a3dfc17-58da-4629-ac6d-5b3b79cf68b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672, 59)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc09a3-dc99-430b-9191-ad921bae3108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f73dcb-5826-4883-8db9-4597a6029a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292ad04-327e-4c81-9f06-4cde6bcc4d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f67b0-153b-43ae-98c0-3089dfb4695a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15a2d4-c1a8-45f9-b9c1-3f7654d15285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac763cb0-09bf-49dc-bad0-ca99b577e356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[': 1750,\n",
       "         '3': 771,\n",
       "         ',': 3500,\n",
       "         ' ': 3500,\n",
       "         '7': 736,\n",
       "         ']': 1750,\n",
       "         '6': 1142,\n",
       "         '0': 650,\n",
       "         '1': 740,\n",
       "         '2': 840,\n",
       "         '5': 360,\n",
       "         '4': 11})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Since the actual data is an image of text, we'll manually transcribe a few lines to demonstrate the process.\n",
    "# In practice, the user would extract the text data using OCR (Optical Character Recognition) tools such as Tesseract.\n",
    "\n",
    "\n",
    "# Convert the example data into a DataFrame\n",
    "df = pd.read_csv(\"./results_[0.1].csv\")\n",
    "\n",
    "# Flatten the list of lists into a single list\n",
    "all_indices = [i for sublist in df['Alpha Max Indices'] for i in sublist]\n",
    "\n",
    "# Count the frequency of each number\n",
    "frequency_counts = Counter(all_indices)\n",
    "\n",
    "frequency_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff025fa-23be-4979-a298-6bd8ecc42a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
