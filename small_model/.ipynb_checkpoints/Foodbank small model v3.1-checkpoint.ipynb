{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0aa7ae9-f9cd-492f-8f87-5e0c63ef709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7bde29-0d3e-4cc2-b0a4-1f1dcdb63807",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # specify ImageNet mean and standard deviation\n",
    "    \"IMG_MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"IMG_STD\": [0.229, 0.224, 0.225],\n",
    "    \"INIT_LR\": 1e-4,\n",
    "    \"NUM_EPOCHS\": 30,\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    # specify the loss weights\n",
    "    \"LABELS_PRDTYPE\": 1.0,\n",
    "    \"LABELS_WEIGHT\": 1.0,\n",
    "    \"LABELS_HALAL\": 1.0,\n",
    "    \"LABELS_HEALTHY\": 1.0,\n",
    "    \"MODEL_PATH\": os.path.sep.join([\"output\", \"detector.pth\"]),\n",
    "    \"LE_PATH_PRDTYPE\": os.path.sep.join([\"output\", \"le_prdtype.pickle\"]),\n",
    "    \"LE_PATH_WEIGHT\": os.path.sep.join([\"output\", \"le_weight.pickle\"]),\n",
    "    \"LE_PATH_HALAL\": os.path.sep.join([\"output\", \"le_halal.pickle\"]),\n",
    "    \"LE_PATH_HEALTHY\": os.path.sep.join([\"output\", \"le_healthy.pickle\"]),\n",
    "    \"PIN_MEMORY\": True if torch.cuda.is_available() else False,\n",
    "    \"DATA_BASE_PATH\": os.path.join('..', 'rshiny', 'www', 'all_images'),\n",
    "    \"NEW_DATA_BASE_PATH\": os.path.join('..', 'small_model', 'new_imgs'),\n",
    "    \"BASE_PATH\": os.path.join('..'),\n",
    "    \"EARLY_STOPPING_PATIENCE\": 5,\n",
    "    'SMALL_MODEL_IMG_SIZE': 60\n",
    "}\n",
    "\n",
    "# create output folder\n",
    "if not os.path.exists(\"output\"):\n",
    "    !mkdir -p {\"output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8415bd71-0c8f-4516-abce-bbcb6473de4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HalalStatus</th>\n",
       "      <th>HealthStatus</th>\n",
       "      <th>new_camera</th>\n",
       "      <th>tag</th>\n",
       "      <th>group</th>\n",
       "      <th>tokeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_20230428_123528_jpg.rf.5687b7b914f6d9aa98c...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_20230428_123522_jpg.rf.204ff37f497f2dce442...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_20230428_123708_jpg.rf.141ecd0cefaea75c0b7...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_20230428_123521_jpg.rf.1069b402272252862ec...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_20230428_123659_jpg.rf.5e1b6c4caabe48cf360...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0  IMG_20230428_123528_jpg.rf.5687b7b914f6d9aa98c...   \n",
       "1  IMG_20230428_123522_jpg.rf.204ff37f497f2dce442...   \n",
       "2  IMG_20230428_123708_jpg.rf.141ecd0cefaea75c0b7...   \n",
       "3  IMG_20230428_123521_jpg.rf.1069b402272252862ec...   \n",
       "4  IMG_20230428_123659_jpg.rf.5e1b6c4caabe48cf360...   \n",
       "\n",
       "                                label ProductType    Weight HalalStatus  \\\n",
       "0  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "1  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "2  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "3  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "4  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "\n",
       "  HealthStatus  new_camera  tag  group  tokeep  \n",
       "0   NonHealthy           0  NaN    1.0    True  \n",
       "1   NonHealthy           0  NaN    1.0    True  \n",
       "2   NonHealthy           0  NaN    2.0   False  \n",
       "3   NonHealthy           0  NaN    1.0    True  \n",
       "4   NonHealthy           0  NaN    2.0   False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_group_df = pd.read_csv(os.path.join(CONFIGS['BASE_PATH'], \"bayesian_model/product_group.csv\"))\n",
    "# prd_group_df['group'] = pd.to_numeric(prd_group_df['group'], errors='coerce')\n",
    "prd_group_df['group'] = prd_group_df['group'].fillna(0)\n",
    "# prd_group_df['tokeep'] = (prd_group_df['group'].astype(int) >= 1) & (prd_group_df['group'].astype(int) <= 7)\n",
    "prd_group_df['tokeep'] = prd_group_df['group'].astype(int) == 1\n",
    "prd_group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fecab9-5ae1-4a6a-b81b-df92cfca1678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6850, 13)\n",
      "(406, 15)\n",
      "(10, 15)\n",
      "(416, 15)\n"
     ]
    }
   ],
   "source": [
    "annotations_0 = pd.read_csv(\"../master_list.csv\")\n",
    "print(annotations_0.shape)\n",
    "annotations_0 = annotations_0[annotations_0['remove']!=1]\n",
    "annotations_0 = pd.merge(annotations_0, prd_group_df[[\"filepath\", \"tokeep\"]], how='left', on='filepath')\n",
    "\n",
    "annotations_1 = annotations_0.loc[annotations_0[\"tokeep\"]==True]\n",
    "annotations_1 = annotations_1[annotations_1['tag'] != 'TestforMode5']\n",
    "annotations_1 = annotations_1.groupby('ProductType').apply(lambda x: x.sample(n=min(len(x), 6))).reset_index(drop=True)\n",
    "# add images whose prod type is not covered\n",
    "a = annotations_0[\"ProductType\"].unique().tolist()\n",
    "b = annotations_1[\"ProductType\"].unique().tolist()\n",
    "missing_prdtype = [i for i in a if i not in b]\n",
    "if len(missing_prdtype) != 0:\n",
    "    # for tmp_prdtype in missing_prdtypemissing_prdtype:\n",
    "    tmp_df = annotations_0[annotations_0['ProductType'].isin(missing_prdtype)]\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    annotations_1_addon = tmp_df.groupby('ProductType').apply(lambda x: x.sample(n=min(len(x), 6))).reset_index(drop=True)\n",
    "    annotations_1 = pd.concat([annotations_1, annotations_1_addon], ignore_index=True)\n",
    "    annotations_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "annotations_1['type'] = \"old\"\n",
    "print(annotations_1.shape)\n",
    "\n",
    "annotations_2 = annotations_0[annotations_0['tag'] == 'TestforMode5']\n",
    "annotations_2.reset_index(drop=True, inplace=True)\n",
    "annotations_2['type'] = \"new\"\n",
    "print(annotations_2.shape)\n",
    "\n",
    "# Concatenate the two dataframes vertically\n",
    "annotations = pd.concat([annotations_1, annotations_2], ignore_index=True)\n",
    "annotations.reset_index(drop=True, inplace=True)\n",
    "print(annotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cb2733-2840-4b15-ba99-195487ae840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves', 'Coffee',\n",
       "       'RiceBrownOthers', 'RiceWhite'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['ProductType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b6bb8c4-a6d0-473f-9921-c560544ee7c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     labels[label_name] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(label_data)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Split the data and labels into training and testing sets\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# split = train_test_split(data, *labels.values(), imagePaths, filenames,\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#                          test_size=0.2, random_state=42, stratify=labels['labels_prdtype'])\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m split \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimagePaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Unpack the data split\u001b[39;00m\n\u001b[1;32m     47\u001b[0m (trainImages, testImages, \u001b[38;5;241m*\u001b[39msplit_labels, trainPaths, testPaths, trainFilenames, testFilenames) \u001b[38;5;241m=\u001b[39m split\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2670\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2666\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2668\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2670\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:1746\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1745\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1746\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2147\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2145\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2152\u001b[0m     )\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[1;32m   2155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# Initialize lists for processed data\n",
    "data, imagePaths, filenames, prdtypes = [], [], [], []\n",
    "\n",
    "# Process each annotation entry\n",
    "for idx, row in annotations.iterrows():\n",
    "    filepath = row[\"filepath\"]\n",
    "    imagePath = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], filepath)\n",
    "    # if row['Type'] == 'old':\n",
    "    #     imagePath = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], filepath)\n",
    "    # else:\n",
    "    #     imagePath = os.path.join(\"/content\", CONFIGS[\"NEW_DATA_BASE_PATH\"], filepath)\n",
    "    image = cv2.imread(imagePath)\n",
    "    \n",
    "    # Preprocess image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (CONFIGS['SMALL_MODEL_IMG_SIZE'], CONFIGS['SMALL_MODEL_IMG_SIZE']))\n",
    "\n",
    "    # Append processed data to lists\n",
    "    data.append(image)\n",
    "    imagePaths.append(imagePath)\n",
    "    # filenames.append(filepath.rsplit('.', 1)[0])\n",
    "    filenames.append(filepath)\n",
    "    prdtypes.append(row[\"ProductType\"])\n",
    "    if row[\"ProductType\"] == \"Coffee\":\n",
    "        break\n",
    "\n",
    "# Convert data to NumPy arrays for machine learning processing\n",
    "labels = {\n",
    "    'labels_prdtype': annotations['ProductType'],\n",
    "    # 'labels_weight': annotations['Weight'],\n",
    "    # 'labels_halal': annotations['HalalStatus'],\n",
    "    # 'labels_healthy': annotations['HealthStatus'],\n",
    "    # 'labels_total': annotations['label']\n",
    "}\n",
    "\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "for label_name, label_data in labels.items():\n",
    "    labels[label_name] = np.array(label_data)\n",
    "\n",
    "# Split the data and labels into training and testing sets\n",
    "split = train_test_split(data, *labels.values(), imagePaths, filenames,\n",
    "                         test_size=0.2, random_state=42, stratify=labels['labels_prdtype'])\n",
    "# split = train_test_split(data, prdtypes, imagePaths, filenames,\n",
    "#                          test_size=0.2, random_state=42, stratify=prdtypes)\n",
    "\n",
    "# Unpack the data split\n",
    "(trainImages, testImages, *split_labels, trainPaths, testPaths, trainFilenames, testFilenames) = split\n",
    "\n",
    "# Create label encoders and transform labels\n",
    "le_prdtype = LabelEncoder()\n",
    "# le_weight = LabelEncoder()\n",
    "# le_halal = LabelEncoder()\n",
    "# le_healthy = LabelEncoder()\n",
    "# le_total = LabelEncoder()\n",
    "\n",
    "trainLabels = {}\n",
    "testLabels = {}\n",
    "\n",
    "# Fit label encoders and transform labels\n",
    "trainLabels['labels_prdtype'] = le_prdtype.fit_transform(split_labels[0])\n",
    "testLabels['labels_prdtype'] = le_prdtype.transform(split_labels[1])\n",
    "\n",
    "# trainLabels['labels_weight'] = le_weight.fit_transform(split_labels[2])\n",
    "# testLabels['labels_weight'] = le_weight.transform(split_labels[3])\n",
    "\n",
    "# trainLabels['labels_halal'] = le_halal.fit_transform(split_labels[4])\n",
    "# testLabels['labels_halal'] = le_halal.transform(split_labels[5])\n",
    "\n",
    "# trainLabels['labels_healthy'] = le_healthy.fit_transform(split_labels[6])\n",
    "# testLabels['labels_healthy'] = le_healthy.transform(split_labels[7])\n",
    "\n",
    "# trainLabels['labels_total'] = le_total.fit_transform(split_labels[8])\n",
    "# testLabels['labels_total'] = le_total.transform(split_labels[9])\n",
    "\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "trainImages, testImages = torch.tensor(trainImages), torch.tensor(testImages)\n",
    "for label_name in labels.keys():\n",
    "    trainLabels[label_name] = torch.tensor(trainLabels[label_name])\n",
    "    testLabels[label_name] = torch.tensor(testLabels[label_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c17bc2d-1dd0-4751-ac90-b3f69fe472cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves', 'Coffee',\n",
       "       'RiceBrownOthers', 'RiceWhite'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['ProductType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6431a709-7fad-4242-bddb-dc21103ed758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'Coffee', 'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RiceBrownOthers', 'RiceWhite',\n",
       "       'RolledOatsInstantOatmeal', 'Salt', 'SoftDrinksOtherReadyToDrink',\n",
       "       'SoupStock', 'Sugar', 'SweetsChocolatesOthers', 'TeaPowderLeaves'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_prdtype.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69589a4f-915b-4cb5-8744-84fd8902c63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Coffee\" in split_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972b8c27-6e91-4db0-9468-4ff887c84a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves', 'Coffee',\n",
       "       'RiceBrownOthers', 'RiceWhite'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(prdtypes).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da471140-89c9-4e07-ab5a-66d299ef209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping from class name to index\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(le_prdtype.classes_)}\n",
    "\n",
    "# Specify your target class name\n",
    "target_class_name = annotations_2['ProductType'].unique()[0]\n",
    "\n",
    "# Initialize weights to 1 for all classes\n",
    "weights = torch.ones(len(class_to_idx))\n",
    "\n",
    "# Set the weight of the target class to 10\n",
    "weights[class_to_idx[target_class_name]] = 10\n",
    "\n",
    "criterion_prdtype = nn.CrossEntropyLoss(weight=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f0617d-27a1-46d6-8eb1-b0de887a744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] total training samples: 332...\n",
      "[INFO] total test samples: 84...\n"
     ]
    }
   ],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    # Initialize the constructor\n",
    "    def __init__(self, images, labels, filenames, transforms=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.filenames = filenames\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Grab the image, labels, and its bounding box coordinates\n",
    "        image = self.images[index]\n",
    "        label_prdtype = self.labels['labels_prdtype'][index]\n",
    "        filename = self.filenames[index]\n",
    "\n",
    "        # Transpose the image such that its channel dimension becomes the leading one\n",
    "        image = image.permute(2, 0, 1)\n",
    "\n",
    "        # Check to see if we have any image transformations to apply and if so, apply them\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Return a tuple of the images, labels, and bounding box coordinates\n",
    "        return (image, label_prdtype, filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.images)\n",
    "\n",
    "# Define normalization and augmentation transforms\n",
    "normalization_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIGS['IMG_MEAN'], std=CONFIGS['IMG_STD'])\n",
    "])\n",
    "\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(20),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n",
    "])\n",
    "\n",
    "# Combine augmentation and normalization for training\n",
    "train_transforms = transforms.Compose([augmentation_transforms, normalization_transforms])\n",
    "test_transforms = normalization_transforms\n",
    "\n",
    "# Create PyTorch datasets\n",
    "trainDS = CustomTensorDataset(trainImages, trainLabels, trainFilenames, transforms=train_transforms)\n",
    "testDS = CustomTensorDataset(testImages, testLabels, testFilenames, transforms=test_transforms)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"[INFO] total training samples: {}...\".format(len(trainDS)))\n",
    "print(\"[INFO] total test samples: {}...\".format(len(testDS)))\n",
    "\n",
    "# Calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDS) // CONFIGS['BATCH_SIZE']\n",
    "valSteps = len(testDS) // CONFIGS['BATCH_SIZE']\n",
    "\n",
    "# Create data loaders\n",
    "# trainLoader = DataLoader(trainDS, batch_size=CONFIGS['BATCH_SIZE'], shuffle=True,\n",
    "#                          num_workers=os.cpu_count(), pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "# testLoader = DataLoader(testDS, batch_size=CONFIGS['BATCH_SIZE'],\n",
    "#                         num_workers=os.cpu_count(), pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "\n",
    "trainLoader = DataLoader(trainDS, batch_size=CONFIGS['BATCH_SIZE'], shuffle=True, num_workers=0, pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "testLoader = DataLoader(testDS, batch_size=CONFIGS['BATCH_SIZE'], num_workers=0, pin_memory=CONFIGS['PIN_MEMORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7dce84-8039-456c-a0fc-1c9d3651dfb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training started...\n",
      "Epoch 1/30\n",
      "train Loss: 4.1449 Acc: 0.0331\n",
      "epochs_no_improve: 0\n",
      "val Loss: 4.0611 Acc: 0.0476\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 2/30\n",
      "train Loss: 3.3600 Acc: 0.0904\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.6774 Acc: 0.0476\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 3/30\n",
      "train Loss: 3.0435 Acc: 0.1687\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.4486 Acc: 0.0952\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 4/30\n",
      "train Loss: 2.6796 Acc: 0.2199\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.2992 Acc: 0.1548\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 5/30\n",
      "train Loss: 2.2729 Acc: 0.3614\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.1060 Acc: 0.1786\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 6/30\n",
      "train Loss: 2.1409 Acc: 0.4247\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.9414 Acc: 0.2500\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 7/30\n",
      "train Loss: 1.8970 Acc: 0.5000\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.8700 Acc: 0.2976\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 8/30\n",
      "train Loss: 1.6228 Acc: 0.5873\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.7326 Acc: 0.2857\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 9/30\n",
      "train Loss: 1.5934 Acc: 0.6084\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.7092 Acc: 0.3571\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 10/30\n",
      "train Loss: 1.3953 Acc: 0.6536\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.6111 Acc: 0.3810\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 11/30\n",
      "train Loss: 1.2784 Acc: 0.6867\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.4800 Acc: 0.4048\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 12/30\n",
      "train Loss: 1.1111 Acc: 0.7620\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.3091 Acc: 0.4286\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 13/30\n",
      "train Loss: 0.9098 Acc: 0.8042\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.2456 Acc: 0.4881\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 14/30\n",
      "train Loss: 0.7725 Acc: 0.8494\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.2800 Acc: 0.4881\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 15/30\n",
      "train Loss: 0.7565 Acc: 0.8494\n",
      "epochs_no_improve: 1\n",
      "val Loss: 2.2452 Acc: 0.4762\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 16/30\n",
      "train Loss: 0.7494 Acc: 0.8404\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.0863 Acc: 0.5000\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 17/30\n",
      "train Loss: 0.6832 Acc: 0.8735\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.0453 Acc: 0.4881\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 18/30\n",
      "train Loss: 0.5980 Acc: 0.8916\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.1133 Acc: 0.4881\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 19/30\n",
      "train Loss: 0.5106 Acc: 0.8946\n",
      "epochs_no_improve: 1\n",
      "val Loss: 2.0112 Acc: 0.4643\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 20/30\n",
      "train Loss: 0.5021 Acc: 0.8825\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.0583 Acc: 0.5357\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 21/30\n",
      "train Loss: 0.4463 Acc: 0.9096\n",
      "epochs_no_improve: 1\n",
      "val Loss: 1.9776 Acc: 0.5595\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 22/30\n",
      "train Loss: 0.4381 Acc: 0.9337\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.9601 Acc: 0.5833\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 23/30\n",
      "train Loss: 0.3416 Acc: 0.9367\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.9263 Acc: 0.5476\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 24/30\n",
      "train Loss: 0.3440 Acc: 0.9458\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.9090 Acc: 0.5714\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 25/30\n",
      "train Loss: 0.3850 Acc: 0.9428\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.9422 Acc: 0.5595\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 26/30\n",
      "train Loss: 0.3047 Acc: 0.9217\n",
      "epochs_no_improve: 1\n",
      "val Loss: 1.9902 Acc: 0.5833\n",
      "tmp epochs_no_improve: 2\n",
      "epochs_no_improve: 2\n",
      "Epoch 27/30\n",
      "train Loss: 0.2688 Acc: 0.9548\n",
      "epochs_no_improve: 2\n",
      "val Loss: 1.9697 Acc: 0.5714\n",
      "tmp epochs_no_improve: 3\n",
      "epochs_no_improve: 3\n",
      "Epoch 28/30\n",
      "train Loss: 0.2690 Acc: 0.9458\n",
      "epochs_no_improve: 3\n",
      "val Loss: 1.9250 Acc: 0.5476\n",
      "tmp epochs_no_improve: 4\n",
      "epochs_no_improve: 4\n",
      "Epoch 29/30\n",
      "train Loss: 0.2880 Acc: 0.9307\n",
      "epochs_no_improve: 4\n",
      "val Loss: 1.9501 Acc: 0.5357\n",
      "tmp epochs_no_improve: 5\n",
      "Early stopping triggered at epoch: 29\n",
      "Model training completed...\n",
      "Time spent: 2.93 mins\n"
     ]
    }
   ],
   "source": [
    "# Define the MultiHeadResNet model\n",
    "class MultiHeadResNet(nn.Module):\n",
    "    def __init__(self, num_classes_prdtype):\n",
    "        super(MultiHeadResNet, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "        # Define custom fully connected layers for each prediction head\n",
    "        self.fc_prdtype = nn.Linear(num_ftrs, num_classes_prdtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        prdtype = self.fc_prdtype(x)\n",
    "        return prdtype\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    corrects = torch.sum(preds == labels.data)\n",
    "    return corrects.double() / labels.size(0)\n",
    "\n",
    "# Training and Validation Loop with Early Stopping\n",
    "def train_model(model, criteria, optimizer, train_loader, test_loader, device, num_epochs=25, early_stopping_patience=10):\n",
    "    criterion_prdtype= criteria\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc_prdtype': [],\n",
    "        'train_acc_overall': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc_prdtype': [],\n",
    "        'val_acc_overall': [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_prdtype = 0\n",
    "            running_corrects_overall = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for inputs, label_prdtype, _ in train_loader if phase == 'train' else test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                label_prdtype = label_prdtype.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_prdtype = model(inputs)\n",
    "                    loss_prdtype = criterion_prdtype(outputs_prdtype, label_prdtype)\n",
    "                    loss = loss_prdtype  # Total loss\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects_prdtype += calculate_accuracy(outputs_prdtype, label_prdtype) * inputs.size(0)\n",
    "                correct_preds_overall = (outputs_prdtype.argmax(1) == label_prdtype)\n",
    "                running_corrects_overall += correct_preds_overall.sum().item()\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_acc_prdtype = running_corrects_prdtype / total_samples\n",
    "            epoch_acc_overall = running_corrects_overall / total_samples\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc_overall))\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    print(\"new loss obtained\")\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f\"tmp epochs_no_improve: {epochs_no_improve}\")\n",
    "\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered at epoch: {}\".format(epoch + 1))\n",
    "                    model.load_state_dict(best_model_wts)\n",
    "                    return model, history\n",
    "            \n",
    "            print(f\"epochs_no_improve: {epochs_no_improve}\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "# Example usage of the function\n",
    "# Assuming CONFIGS, trainLoader, testLoader, etc. are already defined\n",
    "num_classes_prdtype = len(np.unique(trainLabels['labels_prdtype']))\n",
    "\n",
    "custom_resnet_model = MultiHeadResNet(num_classes_prdtype)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_resnet_model = custom_resnet_model.to(device)\n",
    "\n",
    "# criterion_prdtype = nn.CrossEntropyLoss()\n",
    "criterion_prdtype = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer = optim.Adam(custom_resnet_model.parameters(), lr=CONFIGS['INIT_LR'])\n",
    "\n",
    "criteria = criterion_prdtype\n",
    "\n",
    "# Start time\n",
    "print(\"Model training started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model_ft, history = train_model(custom_resnet_model, criteria, optimizer, \n",
    "                                trainLoader, testLoader, device, \n",
    "                                num_epochs=CONFIGS['NUM_EPOCHS'], \n",
    "                                early_stopping_patience=CONFIGS['EARLY_STOPPING_PATIENCE'])\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "print(\"Model training completed...\")\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time spent: {round(execution_time/60,2)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7c51679-7fe1-4459-b105-9692bf06a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving label encoder...\n"
     ]
    }
   ],
   "source": [
    "torch.save(model_ft.state_dict(), 'output/multi_head_model.pth')\n",
    "\n",
    "print(\"[INFO] saving label encoder...\")\n",
    "f = open(CONFIGS[\"LE_PATH_PRDTYPE\"], \"wb\")\n",
    "f.write(pickle.dumps(le_prdtype))\n",
    "f.close()\n",
    "# f = open(CONFIGS[\"LE_PATH_WEIGHT\"], \"wb\")\n",
    "# f.write(pickle.dumps(le_weight))\n",
    "# f.close()\n",
    "# f = open(CONFIGS[\"LE_PATH_HALAL\"], \"wb\")\n",
    "# f.write(pickle.dumps(le_halal))\n",
    "# f.close()\n",
    "# f = open(CONFIGS[\"LE_PATH_HEALTHY\"], \"wb\")\n",
    "# f.write(pickle.dumps(le_healthy))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6de520c9-41e2-4c11-bed9-67caf2609523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracies: {'ProductType': 0.9728915662650602}\n",
      "Test Accuracies: {'ProductType': 0.5714285714285714}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, dataset_size, num_mc_samples=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_counts = {'ProductType': 0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, labels_prdtype, filenames) in data_loader:\n",
    "            images = images.to(CONFIGS['DEVICE'])\n",
    "            labels_prdtype = labels_prdtype.to(CONFIGS['DEVICE'])\n",
    "\n",
    "            # Forward pass\n",
    "            out1 = model(images)\n",
    "\n",
    "            # Store deterministic predictions\n",
    "            det_pred_prdtype = out1.argmax(1)\n",
    "\n",
    "            # Update correct counts for each category\n",
    "            correct_counts['ProductType'] += (out1.argmax(1) == labels_prdtype).float().sum().item()\n",
    "\n",
    "    # Calculate accuracies\n",
    "    accuracies = {key: correct_counts[key] / dataset_size for key in correct_counts}\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracies = evaluate_model(model_ft, trainLoader, len(trainDS))\n",
    "print(f\"Training Accuracies: {train_accuracies}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracies= evaluate_model(model_ft, testLoader, len(testDS))\n",
    "print(f\"Test Accuracies: {test_accuracies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ca685c-9918-42e2-b8eb-e99179fda09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on new images\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "all_filepaths, all_prdlabel_preds = [], []\n",
    "\n",
    "for i in range(len(annotations_2)):\n",
    "    image_path = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], annotations_2['filepath'][i])\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    # Preprocessing steps\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (CONFIGS['SMALL_MODEL_IMG_SIZE'], CONFIGS['SMALL_MODEL_IMG_SIZE']))\n",
    "    frame = frame.transpose((2, 0, 1))\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    frame = test_transforms(frame).unsqueeze(0).to(CONFIGS['DEVICE'])\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        out1 = model_ft(frame)\n",
    "    \n",
    "    # Extract and store the results\n",
    "    tmp_pred = out1.argmax(1)\n",
    "    tmp_pred_label = le_prdtype.inverse_transform([tmp_pred])\n",
    "    all_prdlabel_preds.append(tmp_pred_label[0])\n",
    "\n",
    "\n",
    "print(\"Accuracy on new images\")\n",
    "print(sum(all_prdlabel_preds == annotations_2['ProductType']) / len(annotations_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e78a4cd-8cf4-455a-b6cd-65284d08908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 15)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a319c9f-c8b2-4cef-9a04-88699c5a1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = []\n",
    "\n",
    "for i in range(len(annotations)):\n",
    "    image_path = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], annotations['filepath'][i])\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    # Preprocessing steps\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (CONFIGS['SMALL_MODEL_IMG_SIZE'], CONFIGS['SMALL_MODEL_IMG_SIZE']))\n",
    "    frame = frame.transpose((2, 0, 1))\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    frame = test_transforms(frame).unsqueeze(0).to(CONFIGS['DEVICE'])\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        out1 = model_ft(frame)\n",
    "    \n",
    "    # Extract and store the results\n",
    "    all_logits.append(out1[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea530d84-aeed-4030-8e64-a552c33bc1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>label</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HalalStatus</th>\n",
       "      <th>HealthStatus</th>\n",
       "      <th>...</th>\n",
       "      <th>Potatochips</th>\n",
       "      <th>RiceBrownOthers</th>\n",
       "      <th>RiceWhite</th>\n",
       "      <th>RolledOatsInstantOatmeal</th>\n",
       "      <th>Salt</th>\n",
       "      <th>SoftDrinksOtherReadyToDrink</th>\n",
       "      <th>SoupStock</th>\n",
       "      <th>Sugar</th>\n",
       "      <th>SweetsChocolatesOthers</th>\n",
       "      <th>TeaPowderLeaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023_10_25_12_8_59_18082.jpg</td>\n",
       "      <td>229</td>\n",
       "      <td>137</td>\n",
       "      <td>542</td>\n",
       "      <td>528</td>\n",
       "      <td>BabyMilkPowder_900-999g_NonHalal_NonHealthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>900-999g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.832793</td>\n",
       "      <td>0.179282</td>\n",
       "      <td>-1.552825</td>\n",
       "      <td>-1.731135</td>\n",
       "      <td>-2.577919</td>\n",
       "      <td>-2.905977</td>\n",
       "      <td>1.590636</td>\n",
       "      <td>-2.662760</td>\n",
       "      <td>0.231224</td>\n",
       "      <td>-2.122313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023_10_25_11_50_16_569076.jpg</td>\n",
       "      <td>266</td>\n",
       "      <td>207</td>\n",
       "      <td>506</td>\n",
       "      <td>528</td>\n",
       "      <td>BabyMilkPowder_400-499g_Halal_NonHealthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.724326</td>\n",
       "      <td>-0.745058</td>\n",
       "      <td>-1.120891</td>\n",
       "      <td>-1.863481</td>\n",
       "      <td>-0.882022</td>\n",
       "      <td>-1.300297</td>\n",
       "      <td>-1.290063</td>\n",
       "      <td>-1.396358</td>\n",
       "      <td>-1.085164</td>\n",
       "      <td>-0.652122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_5324_jpeg.rf.dc36c53f6a02456f3aa98ba4f2507...</td>\n",
       "      <td>200</td>\n",
       "      <td>1100</td>\n",
       "      <td>2550</td>\n",
       "      <td>3047</td>\n",
       "      <td>BabyMilkPowder_700-799g_Halal_Healthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>700-799g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.596636</td>\n",
       "      <td>-2.524034</td>\n",
       "      <td>-1.051161</td>\n",
       "      <td>-0.338299</td>\n",
       "      <td>-1.130502</td>\n",
       "      <td>-3.283288</td>\n",
       "      <td>1.195992</td>\n",
       "      <td>-1.250219</td>\n",
       "      <td>0.782571</td>\n",
       "      <td>1.561377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023_8_11_11_39_59_72262_png.rf.00092e3059cd90...</td>\n",
       "      <td>263</td>\n",
       "      <td>86</td>\n",
       "      <td>571</td>\n",
       "      <td>625</td>\n",
       "      <td>BabyMilkPowder_700-799g_Halal_Healthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>700-799g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.320966</td>\n",
       "      <td>-0.822793</td>\n",
       "      <td>-1.708590</td>\n",
       "      <td>1.181575</td>\n",
       "      <td>-0.901647</td>\n",
       "      <td>-2.978112</td>\n",
       "      <td>-0.446639</td>\n",
       "      <td>-0.658142</td>\n",
       "      <td>0.549129</td>\n",
       "      <td>-0.013531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023_10_25_11_45_12_700039.jpg</td>\n",
       "      <td>229</td>\n",
       "      <td>83</td>\n",
       "      <td>556</td>\n",
       "      <td>528</td>\n",
       "      <td>BabyMilkPowder_800-899g_Halal_Healthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>800-899g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.252228</td>\n",
       "      <td>-0.329702</td>\n",
       "      <td>-0.981007</td>\n",
       "      <td>-1.909498</td>\n",
       "      <td>-1.910300</td>\n",
       "      <td>-3.047790</td>\n",
       "      <td>-1.013962</td>\n",
       "      <td>-3.674145</td>\n",
       "      <td>0.023251</td>\n",
       "      <td>-1.106590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  xmin  ymin  xmax  ymax  \\\n",
       "0                       2023_10_25_12_8_59_18082.jpg   229   137   542   528   \n",
       "1                     2023_10_25_11_50_16_569076.jpg   266   207   506   528   \n",
       "2  IMG_5324_jpeg.rf.dc36c53f6a02456f3aa98ba4f2507...   200  1100  2550  3047   \n",
       "3  2023_8_11_11_39_59_72262_png.rf.00092e3059cd90...   263    86   571   625   \n",
       "4                     2023_10_25_11_45_12_700039.jpg   229    83   556   528   \n",
       "\n",
       "                                         label     ProductType    Weight  \\\n",
       "0  BabyMilkPowder_900-999g_NonHalal_NonHealthy  BabyMilkPowder  900-999g   \n",
       "1     BabyMilkPowder_400-499g_Halal_NonHealthy  BabyMilkPowder  400-499g   \n",
       "2        BabyMilkPowder_700-799g_Halal_Healthy  BabyMilkPowder  700-799g   \n",
       "3        BabyMilkPowder_700-799g_Halal_Healthy  BabyMilkPowder  700-799g   \n",
       "4        BabyMilkPowder_800-899g_Halal_Healthy  BabyMilkPowder  800-899g   \n",
       "\n",
       "  HalalStatus HealthStatus  ...  Potatochips RiceBrownOthers  RiceWhite  \\\n",
       "0    NonHalal   NonHealthy  ...    -1.832793        0.179282  -1.552825   \n",
       "1       Halal   NonHealthy  ...    -1.724326       -0.745058  -1.120891   \n",
       "2       Halal      Healthy  ...    -3.596636       -2.524034  -1.051161   \n",
       "3       Halal      Healthy  ...    -2.320966       -0.822793  -1.708590   \n",
       "4       Halal      Healthy  ...    -3.252228       -0.329702  -0.981007   \n",
       "\n",
       "  RolledOatsInstantOatmeal      Salt SoftDrinksOtherReadyToDrink  SoupStock  \\\n",
       "0                -1.731135 -2.577919                   -2.905977   1.590636   \n",
       "1                -1.863481 -0.882022                   -1.300297  -1.290063   \n",
       "2                -0.338299 -1.130502                   -3.283288   1.195992   \n",
       "3                 1.181575 -0.901647                   -2.978112  -0.446639   \n",
       "4                -1.909498 -1.910300                   -3.047790  -1.013962   \n",
       "\n",
       "      Sugar  SweetsChocolatesOthers  TeaPowderLeaves  \n",
       "0 -2.662760                0.231224        -2.122313  \n",
       "1 -1.396358               -1.085164        -0.652122  \n",
       "2 -1.250219                0.782571         1.561377  \n",
       "3 -0.658142                0.549129        -0.013531  \n",
       "4 -3.674145                0.023251        -1.106590  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert list of lists to DataFrame\n",
    "all_logits_df = pd.DataFrame(all_logits)\n",
    "all_logits_df.columns = le_prdtype.classes_\n",
    "all_logits_df['pred_prdtype'] = all_logits_df.idxmax(axis=1)\n",
    "target_column = 'pred_prdtype'\n",
    "columns = [target_column] + [col for col in all_logits_df.columns if col != target_column]\n",
    "all_logits_df = all_logits_df[columns]\n",
    "\n",
    "final_df = pd.concat([annotations, all_logits_df], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5d3c44-14e7-4806-9dab-febf507eebc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9110576923076923"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_df['ProductType'] == final_df['pred_prdtype']) / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e3d34e3-942f-4579-93e5-5f912149d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('new_imgs_results_small_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7002b9e-4c36-4cb9-abd1-dd64145b33fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filepath', 'xmin', 'ymin', 'xmax', 'ymax', 'label', 'ProductType',\n",
       "       'Weight', 'HalalStatus', 'HealthStatus', 'new_camera', 'tag', 'remove',\n",
       "       'tokeep', 'type', 'pred_prdtype', 'BabyMilkPowder', 'Babyfood',\n",
       "       'BeehoonVermicelli', 'BiscuitsCrackersCookies', 'BreakfastCereals',\n",
       "       'CannedBakedBeans', 'CannedBeefOtherMeats', 'CannedBraisedPeanuts',\n",
       "       'CannedChicken', 'CannedFruits', 'CannedMushrooms',\n",
       "       'CannedPacketCreamersSweet', 'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'Coffee', 'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink', 'Flour',\n",
       "       'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood', 'OtherHotBeveragesPowder',\n",
       "       'OtherNoodles', 'OtherSauceDressing', 'OtherSpreads', 'Pasta',\n",
       "       'PastaSauce', 'PeanutButter', 'Potatochips', 'RiceBrownOthers',\n",
       "       'RiceWhite', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f598c285-4e6d-48ac-8496-7e87429fdc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves', 'Coffee',\n",
       "       'RiceBrownOthers', 'RiceWhite'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['ProductType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff025fa-23be-4979-a298-6bd8ecc42a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pred_prdtype',\n",
       " 'BabyMilkPowder',\n",
       " 'Babyfood',\n",
       " 'BeehoonVermicelli',\n",
       " 'BiscuitsCrackersCookies',\n",
       " 'BreakfastCereals',\n",
       " 'CannedBakedBeans',\n",
       " 'CannedBeefOtherMeats',\n",
       " 'CannedBraisedPeanuts',\n",
       " 'CannedChicken',\n",
       " 'CannedFruits',\n",
       " 'CannedMushrooms',\n",
       " 'CannedPacketCreamersSweet',\n",
       " 'CannedPickles',\n",
       " 'CannedPorkLunchronMeat',\n",
       " 'CannedSardinesMackerel',\n",
       " 'CannedSoup',\n",
       " 'CannedTunaDace',\n",
       " 'CannedVegetarianFood',\n",
       " 'ChocolateMaltPowder',\n",
       " 'ChocolateSpread',\n",
       " 'Coffee',\n",
       " 'CoffeePowder',\n",
       " 'CoffeeTeaDrink',\n",
       " 'CookingCreamMilk',\n",
       " 'CookingPastePowder',\n",
       " 'DarkSoySauce',\n",
       " 'DriedBeans',\n",
       " 'DriedFruits',\n",
       " 'DriedMeatSeafood',\n",
       " 'DriedVegetables',\n",
       " 'FlavoredMilkDrink',\n",
       " 'Flour',\n",
       " 'FruitJuiceDrink',\n",
       " 'HerbsSpices',\n",
       " 'InstantMeals',\n",
       " 'InstantNoodlesMultipack',\n",
       " 'InstantNoodlesSingle',\n",
       " 'Jam',\n",
       " 'Kaya',\n",
       " 'KetchupChilliSauce',\n",
       " 'LightSoySauce',\n",
       " 'MaternalMilkPowder',\n",
       " 'MilkDrink',\n",
       " 'MilkPowder',\n",
       " 'Nuts',\n",
       " 'Oil',\n",
       " 'OtherBakingNeeds',\n",
       " 'OtherCannedBeansPeasNuts',\n",
       " 'OtherCannedSeafood',\n",
       " 'OtherCannedVegetables',\n",
       " 'OtherDriedFood',\n",
       " 'OtherHotBeveragesPowder',\n",
       " 'OtherNoodles',\n",
       " 'OtherSauceDressing',\n",
       " 'OtherSpreads',\n",
       " 'Pasta',\n",
       " 'PastaSauce',\n",
       " 'PeanutButter',\n",
       " 'Potatochips',\n",
       " 'RiceBrownOthers',\n",
       " 'RiceWhite',\n",
       " 'RolledOatsInstantOatmeal',\n",
       " 'Salt',\n",
       " 'SoftDrinksOtherReadyToDrink',\n",
       " 'SoupStock',\n",
       " 'Sugar',\n",
       " 'SweetsChocolatesOthers',\n",
       " 'TeaPowderLeaves']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72706b-9da3-4cc9-8da6-1c6ace5df235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
