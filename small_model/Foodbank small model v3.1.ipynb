{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0aa7ae9-f9cd-492f-8f87-5e0c63ef709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7bde29-0d3e-4cc2-b0a4-1f1dcdb63807",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # specify ImageNet mean and standard deviation\n",
    "    \"IMG_MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"IMG_STD\": [0.229, 0.224, 0.225],\n",
    "    \"INIT_LR\": 1e-4,\n",
    "    \"NUM_EPOCHS\": 30,\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    # specify the loss weights\n",
    "    \"LABELS_PRDTYPE\": 1.0,\n",
    "    \"LABELS_WEIGHT\": 1.0,\n",
    "    \"LABELS_HALAL\": 1.0,\n",
    "    \"LABELS_HEALTHY\": 1.0,\n",
    "    \"MODEL_PATH\": os.path.sep.join([\"output\", \"detector.pth\"]),\n",
    "    \"LE_PATH_PRDTYPE\": os.path.sep.join([\"output\", \"le_prdtype.pickle\"]),\n",
    "    \"LE_PATH_WEIGHT\": os.path.sep.join([\"output\", \"le_weight.pickle\"]),\n",
    "    \"LE_PATH_HALAL\": os.path.sep.join([\"output\", \"le_halal.pickle\"]),\n",
    "    \"LE_PATH_HEALTHY\": os.path.sep.join([\"output\", \"le_healthy.pickle\"]),\n",
    "    \"PIN_MEMORY\": True if torch.cuda.is_available() else False,\n",
    "    \"DATA_BASE_PATH\": os.path.join('..', 'rshiny', 'www', 'all_images'),\n",
    "    \"NEW_DATA_BASE_PATH\": os.path.join('..', 'small_model', 'new_imgs'),\n",
    "    \"BASE_PATH\": os.path.join('..'),\n",
    "    \"EARLY_STOPPING_PATIENCE\": 5,\n",
    "    'SMALL_MODEL_IMG_SIZE': 60\n",
    "}\n",
    "\n",
    "# create output folder\n",
    "if not os.path.exists(\"output\"):\n",
    "    !mkdir -p {\"output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8415bd71-0c8f-4516-abce-bbcb6473de4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HalalStatus</th>\n",
       "      <th>HealthStatus</th>\n",
       "      <th>new_camera</th>\n",
       "      <th>tag</th>\n",
       "      <th>group</th>\n",
       "      <th>tokeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_20230428_123528_jpg.rf.5687b7b914f6d9aa98c...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_20230428_123522_jpg.rf.204ff37f497f2dce442...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_20230428_123708_jpg.rf.141ecd0cefaea75c0b7...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_20230428_123521_jpg.rf.1069b402272252862ec...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_20230428_123659_jpg.rf.5e1b6c4caabe48cf360...</td>\n",
       "      <td>Sugar_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  \\\n",
       "0  IMG_20230428_123528_jpg.rf.5687b7b914f6d9aa98c...   \n",
       "1  IMG_20230428_123522_jpg.rf.204ff37f497f2dce442...   \n",
       "2  IMG_20230428_123708_jpg.rf.141ecd0cefaea75c0b7...   \n",
       "3  IMG_20230428_123521_jpg.rf.1069b402272252862ec...   \n",
       "4  IMG_20230428_123659_jpg.rf.5e1b6c4caabe48cf360...   \n",
       "\n",
       "                                label ProductType    Weight HalalStatus  \\\n",
       "0  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "1  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "2  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "3  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "4  Sugar_400-499g_NonHalal_NonHealthy       Sugar  400-499g    NonHalal   \n",
       "\n",
       "  HealthStatus  new_camera  tag  group  tokeep  \n",
       "0   NonHealthy           0  NaN    1.0    True  \n",
       "1   NonHealthy           0  NaN    1.0    True  \n",
       "2   NonHealthy           0  NaN    2.0   False  \n",
       "3   NonHealthy           0  NaN    1.0    True  \n",
       "4   NonHealthy           0  NaN    2.0   False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd_group_df = pd.read_csv(os.path.join(CONFIGS['BASE_PATH'], \"bayesian_model/product_group.csv\"))\n",
    "# prd_group_df['group'] = pd.to_numeric(prd_group_df['group'], errors='coerce')\n",
    "prd_group_df['group'] = prd_group_df['group'].fillna(0)\n",
    "# prd_group_df['tokeep'] = (prd_group_df['group'].astype(int) >= 1) & (prd_group_df['group'].astype(int) <= 7)\n",
    "prd_group_df['tokeep'] = prd_group_df['group'].astype(int) == 1\n",
    "prd_group_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fecab9-5ae1-4a6a-b81b-df92cfca1678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6850, 13)\n",
      "(400, 15)\n",
      "(10, 15)\n",
      "(410, 15)\n"
     ]
    }
   ],
   "source": [
    "annotations_0 = pd.read_csv(\"../master_list.csv\")\n",
    "print(annotations_0.shape)\n",
    "annotations_0 = annotations_0[annotations_0['remove']!=1]\n",
    "annotations_0 = pd.merge(annotations_0, prd_group_df[[\"filepath\", \"tokeep\"]], how='left', on='filepath')\n",
    "\n",
    "annotations_1 = annotations_0.loc[annotations_0[\"tokeep\"]==True]\n",
    "annotations_1 = annotations_1[annotations_1['tag'] != 'TestforMode5']\n",
    "annotations_1 = annotations_1.groupby('ProductType').apply(lambda x: x.sample(n=min(len(x), 6))).reset_index(drop=True)\n",
    "# add images whose prod type is not covered\n",
    "a = annotations_0[\"ProductType\"].unique().tolist()\n",
    "b = annotations_1[\"ProductType\"].unique().tolist()\n",
    "missing_prdtype = [i for i in a if i not in b]\n",
    "if len(missing_prdtype) != 0:\n",
    "    # for tmp_prdtype in missing_prdtypemissing_prdtype:\n",
    "    tmp_df = annotations_0[annotations_0['ProductType'].isin(missing_prdtype)]\n",
    "    tmp_df.reset_index(drop=True, inplace=True)\n",
    "    annotations_1_addon = tmp_df.groupby('ProductType').apply(lambda x: x.sample(n=min(len(x), 6))).reset_index(drop=True)\n",
    "    annotations_1 = pd.concat([annotations_1, annotations_1_addon], ignore_index=True)\n",
    "    annotations_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "annotations_1['type'] = \"old\"\n",
    "print(annotations_1.shape)\n",
    "\n",
    "annotations_2 = annotations_0[annotations_0['tag'] == 'TestforMode5']\n",
    "annotations_2.reset_index(drop=True, inplace=True)\n",
    "annotations_2['type'] = \"new\"\n",
    "print(annotations_2.shape)\n",
    "\n",
    "# Concatenate the two dataframes vertically\n",
    "annotations = pd.concat([annotations_1, annotations_2], ignore_index=True)\n",
    "annotations.reset_index(drop=True, inplace=True)\n",
    "print(annotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cb2733-2840-4b15-ba99-195487ae840f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves', 'RiceBrownOthers',\n",
       "       'RiceWhite'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['ProductType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6bb8c4-a6d0-473f-9921-c560544ee7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for processed data\n",
    "data, imagePaths, filenames, prdtypes = [], [], [], []\n",
    "\n",
    "# Process each annotation entry\n",
    "for idx, row in annotations.iterrows():\n",
    "    filepath = row[\"filepath\"]\n",
    "    imagePath = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], filepath)\n",
    "    # if row['Type'] == 'old':\n",
    "    #     imagePath = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], filepath)\n",
    "    # else:\n",
    "    #     imagePath = os.path.join(\"/content\", CONFIGS[\"NEW_DATA_BASE_PATH\"], filepath)\n",
    "    image = cv2.imread(imagePath)\n",
    "    \n",
    "    # Preprocess image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (CONFIGS['SMALL_MODEL_IMG_SIZE'], CONFIGS['SMALL_MODEL_IMG_SIZE']))\n",
    "\n",
    "    # Append processed data to lists\n",
    "    data.append(image)\n",
    "    imagePaths.append(imagePath)\n",
    "    # filenames.append(filepath.rsplit('.', 1)[0])\n",
    "    filenames.append(filepath)\n",
    "    prdtypes.append(row[\"ProductType\"])\n",
    "    # if row[\"ProductType\"] == \"Coffee\":\n",
    "    #     break\n",
    "\n",
    "# Convert data to NumPy arrays for machine learning processing\n",
    "labels = {\n",
    "    'labels_prdtype': annotations['ProductType'],\n",
    "    # 'labels_weight': annotations['Weight'],\n",
    "    # 'labels_halal': annotations['HalalStatus'],\n",
    "    # 'labels_healthy': annotations['HealthStatus'],\n",
    "    # 'labels_total': annotations['label']\n",
    "}\n",
    "\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "for label_name, label_data in labels.items():\n",
    "    labels[label_name] = np.array(label_data)\n",
    "\n",
    "# Split the data and labels into training and testing sets\n",
    "split = train_test_split(data, *labels.values(), imagePaths, filenames,\n",
    "                         test_size=0.2, random_state=42, stratify=labels['labels_prdtype'])\n",
    "# split = train_test_split(data, prdtypes, imagePaths, filenames,\n",
    "#                          test_size=0.2, random_state=42, stratify=prdtypes)\n",
    "\n",
    "# Unpack the data split\n",
    "(trainImages, testImages, *split_labels, trainPaths, testPaths, trainFilenames, testFilenames) = split\n",
    "\n",
    "# Create label encoders and transform labels\n",
    "le_prdtype = LabelEncoder()\n",
    "# le_weight = LabelEncoder()\n",
    "# le_halal = LabelEncoder()\n",
    "# le_healthy = LabelEncoder()\n",
    "# le_total = LabelEncoder()\n",
    "\n",
    "trainLabels = {}\n",
    "testLabels = {}\n",
    "\n",
    "# Fit label encoders and transform labels\n",
    "trainLabels['labels_prdtype'] = le_prdtype.fit_transform(split_labels[0])\n",
    "testLabels['labels_prdtype'] = le_prdtype.transform(split_labels[1])\n",
    "\n",
    "# trainLabels['labels_weight'] = le_weight.fit_transform(split_labels[2])\n",
    "# testLabels['labels_weight'] = le_weight.transform(split_labels[3])\n",
    "\n",
    "# trainLabels['labels_halal'] = le_halal.fit_transform(split_labels[4])\n",
    "# testLabels['labels_halal'] = le_halal.transform(split_labels[5])\n",
    "\n",
    "# trainLabels['labels_healthy'] = le_healthy.fit_transform(split_labels[6])\n",
    "# testLabels['labels_healthy'] = le_healthy.transform(split_labels[7])\n",
    "\n",
    "# trainLabels['labels_total'] = le_total.fit_transform(split_labels[8])\n",
    "# testLabels['labels_total'] = le_total.transform(split_labels[9])\n",
    "\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "trainImages, testImages = torch.tensor(trainImages), torch.tensor(testImages)\n",
    "for label_name in labels.keys():\n",
    "    trainLabels[label_name] = torch.tensor(trainLabels[label_name])\n",
    "    testLabels[label_name] = torch.tensor(testLabels[label_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6431a709-7fad-4242-bddb-dc21103ed758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RiceBrownOthers', 'RiceWhite',\n",
       "       'RolledOatsInstantOatmeal', 'Salt', 'SoftDrinksOtherReadyToDrink',\n",
       "       'SoupStock', 'Sugar', 'SweetsChocolatesOthers', 'TeaPowderLeaves'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_prdtype.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69589a4f-915b-4cb5-8744-84fd8902c63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Coffee\" in split_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "972b8c27-6e91-4db0-9468-4ff887c84a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves', 'RiceBrownOthers',\n",
       "       'RiceWhite'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(prdtypes).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da471140-89c9-4e07-ab5a-66d299ef209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping from class name to index\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(le_prdtype.classes_)}\n",
    "\n",
    "# Specify your target class name\n",
    "target_class_name = annotations_2['ProductType'].unique()[0]\n",
    "\n",
    "# Initialize weights to 1 for all classes\n",
    "weights = torch.ones(len(class_to_idx))\n",
    "\n",
    "# Set the weight of the target class to 10\n",
    "weights[class_to_idx[target_class_name]] = 10\n",
    "\n",
    "criterion_prdtype = nn.CrossEntropyLoss(weight=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f0617d-27a1-46d6-8eb1-b0de887a744c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] total training samples: 328...\n",
      "[INFO] total test samples: 82...\n"
     ]
    }
   ],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    # Initialize the constructor\n",
    "    def __init__(self, images, labels, filenames, transforms=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.filenames = filenames\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Grab the image, labels, and its bounding box coordinates\n",
    "        image = self.images[index]\n",
    "        label_prdtype = self.labels['labels_prdtype'][index]\n",
    "        filename = self.filenames[index]\n",
    "\n",
    "        # Transpose the image such that its channel dimension becomes the leading one\n",
    "        image = image.permute(2, 0, 1)\n",
    "\n",
    "        # Check to see if we have any image transformations to apply and if so, apply them\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Return a tuple of the images, labels, and bounding box coordinates\n",
    "        return (image, label_prdtype, filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.images)\n",
    "\n",
    "# Define normalization and augmentation transforms\n",
    "normalization_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIGS['IMG_MEAN'], std=CONFIGS['IMG_STD'])\n",
    "])\n",
    "\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(20),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n",
    "])\n",
    "\n",
    "# Combine augmentation and normalization for training\n",
    "train_transforms = transforms.Compose([augmentation_transforms, normalization_transforms])\n",
    "test_transforms = normalization_transforms\n",
    "\n",
    "# Create PyTorch datasets\n",
    "trainDS = CustomTensorDataset(trainImages, trainLabels, trainFilenames, transforms=train_transforms)\n",
    "testDS = CustomTensorDataset(testImages, testLabels, testFilenames, transforms=test_transforms)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"[INFO] total training samples: {}...\".format(len(trainDS)))\n",
    "print(\"[INFO] total test samples: {}...\".format(len(testDS)))\n",
    "\n",
    "# Calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDS) // CONFIGS['BATCH_SIZE']\n",
    "valSteps = len(testDS) // CONFIGS['BATCH_SIZE']\n",
    "\n",
    "# Create data loaders\n",
    "# trainLoader = DataLoader(trainDS, batch_size=CONFIGS['BATCH_SIZE'], shuffle=True,\n",
    "#                          num_workers=os.cpu_count(), pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "# testLoader = DataLoader(testDS, batch_size=CONFIGS['BATCH_SIZE'],\n",
    "#                         num_workers=os.cpu_count(), pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "\n",
    "trainLoader = DataLoader(trainDS, batch_size=CONFIGS['BATCH_SIZE'], shuffle=True, num_workers=0, pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "testLoader = DataLoader(testDS, batch_size=CONFIGS['BATCH_SIZE'], num_workers=0, pin_memory=CONFIGS['PIN_MEMORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc7dce84-8039-456c-a0fc-1c9d3651dfb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training started...\n",
      "Epoch 1/30\n",
      "train Loss: 4.2768 Acc: 0.0213\n",
      "epochs_no_improve: 0\n",
      "val Loss: 4.1411 Acc: 0.0488\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 2/30\n",
      "train Loss: 3.4450 Acc: 0.0671\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.5249 Acc: 0.0732\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 3/30\n",
      "train Loss: 2.9297 Acc: 0.1585\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.2825 Acc: 0.1585\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 4/30\n",
      "train Loss: 2.6465 Acc: 0.2561\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.0435 Acc: 0.1951\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 5/30\n",
      "train Loss: 2.5406 Acc: 0.2927\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.8684 Acc: 0.2317\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 6/30\n",
      "train Loss: 2.2575 Acc: 0.3933\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.7588 Acc: 0.2927\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 7/30\n",
      "train Loss: 1.9424 Acc: 0.5030\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.6489 Acc: 0.3415\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 8/30\n",
      "train Loss: 1.7352 Acc: 0.5671\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.4979 Acc: 0.3780\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 9/30\n",
      "train Loss: 1.5668 Acc: 0.6037\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.4145 Acc: 0.3902\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 10/30\n",
      "train Loss: 1.4044 Acc: 0.6372\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.3287 Acc: 0.4268\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 11/30\n",
      "train Loss: 1.4016 Acc: 0.6402\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.2567 Acc: 0.4268\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 12/30\n",
      "train Loss: 1.1622 Acc: 0.7043\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.0673 Acc: 0.5000\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 13/30\n",
      "train Loss: 1.0793 Acc: 0.7591\n",
      "epochs_no_improve: 0\n",
      "val Loss: 2.0730 Acc: 0.5488\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 14/30\n",
      "train Loss: 0.9819 Acc: 0.7622\n",
      "epochs_no_improve: 1\n",
      "val Loss: 2.0030 Acc: 0.5244\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 15/30\n",
      "train Loss: 0.8224 Acc: 0.8262\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.9068 Acc: 0.5610\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 16/30\n",
      "train Loss: 0.8190 Acc: 0.8110\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.8579 Acc: 0.5854\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 17/30\n",
      "train Loss: 0.6607 Acc: 0.8780\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.7450 Acc: 0.5854\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 18/30\n",
      "train Loss: 0.6648 Acc: 0.8567\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.7058 Acc: 0.5854\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 19/30\n",
      "train Loss: 0.6336 Acc: 0.8567\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.6997 Acc: 0.6220\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 20/30\n",
      "train Loss: 0.5305 Acc: 0.8811\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.6217 Acc: 0.6220\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 21/30\n",
      "train Loss: 0.4354 Acc: 0.9268\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.5799 Acc: 0.6341\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 22/30\n",
      "train Loss: 0.3840 Acc: 0.9360\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.5941 Acc: 0.5854\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 23/30\n",
      "train Loss: 0.4532 Acc: 0.8933\n",
      "epochs_no_improve: 1\n",
      "val Loss: 1.5464 Acc: 0.6220\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 24/30\n",
      "train Loss: 0.4509 Acc: 0.9177\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.5102 Acc: 0.6341\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 25/30\n",
      "train Loss: 0.3548 Acc: 0.9360\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.5112 Acc: 0.6463\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 26/30\n",
      "train Loss: 0.3620 Acc: 0.9421\n",
      "epochs_no_improve: 1\n",
      "val Loss: 1.4820 Acc: 0.6220\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 27/30\n",
      "train Loss: 0.2567 Acc: 0.9604\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.4986 Acc: 0.6098\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 28/30\n",
      "train Loss: 0.3187 Acc: 0.9329\n",
      "epochs_no_improve: 1\n",
      "val Loss: 1.4781 Acc: 0.6098\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 29/30\n",
      "train Loss: 0.2827 Acc: 0.9360\n",
      "epochs_no_improve: 0\n",
      "val Loss: 1.4899 Acc: 0.5976\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 30/30\n",
      "train Loss: 0.2990 Acc: 0.9299\n",
      "epochs_no_improve: 1\n",
      "val Loss: 1.4303 Acc: 0.6098\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Model training completed...\n",
      "Time spent: 2.99 mins\n"
     ]
    }
   ],
   "source": [
    "# Define the MultiHeadResNet model\n",
    "class MultiHeadResNet(nn.Module):\n",
    "    def __init__(self, num_classes_prdtype):\n",
    "        super(MultiHeadResNet, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "        # Define custom fully connected layers for each prediction head\n",
    "        self.fc_prdtype = nn.Linear(num_ftrs, num_classes_prdtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        prdtype = self.fc_prdtype(x)\n",
    "        return prdtype\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    corrects = torch.sum(preds == labels.data)\n",
    "    return corrects.double() / labels.size(0)\n",
    "\n",
    "# Training and Validation Loop with Early Stopping\n",
    "def train_model(model, criteria, optimizer, train_loader, test_loader, device, num_epochs=25, early_stopping_patience=10):\n",
    "    criterion_prdtype= criteria\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc_prdtype': [],\n",
    "        'train_acc_overall': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc_prdtype': [],\n",
    "        'val_acc_overall': [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_prdtype = 0\n",
    "            running_corrects_overall = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for inputs, label_prdtype, _ in train_loader if phase == 'train' else test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                label_prdtype = label_prdtype.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_prdtype = model(inputs)\n",
    "                    loss_prdtype = criterion_prdtype(outputs_prdtype, label_prdtype)\n",
    "                    loss = loss_prdtype  # Total loss\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects_prdtype += calculate_accuracy(outputs_prdtype, label_prdtype) * inputs.size(0)\n",
    "                correct_preds_overall = (outputs_prdtype.argmax(1) == label_prdtype)\n",
    "                running_corrects_overall += correct_preds_overall.sum().item()\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_acc_prdtype = running_corrects_prdtype / total_samples\n",
    "            epoch_acc_overall = running_corrects_overall / total_samples\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc_overall))\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    print(\"new loss obtained\")\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f\"tmp epochs_no_improve: {epochs_no_improve}\")\n",
    "\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered at epoch: {}\".format(epoch + 1))\n",
    "                    model.load_state_dict(best_model_wts)\n",
    "                    return model, history\n",
    "            \n",
    "            print(f\"epochs_no_improve: {epochs_no_improve}\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "# Example usage of the function\n",
    "# Assuming CONFIGS, trainLoader, testLoader, etc. are already defined\n",
    "num_classes_prdtype = len(np.unique(trainLabels['labels_prdtype']))\n",
    "\n",
    "custom_resnet_model = MultiHeadResNet(num_classes_prdtype)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_resnet_model = custom_resnet_model.to(device)\n",
    "\n",
    "# criterion_prdtype = nn.CrossEntropyLoss()\n",
    "criterion_prdtype = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "optimizer = optim.Adam(custom_resnet_model.parameters(), lr=CONFIGS['INIT_LR'])\n",
    "\n",
    "criteria = criterion_prdtype\n",
    "\n",
    "# Start time\n",
    "print(\"Model training started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model_ft, history = train_model(custom_resnet_model, criteria, optimizer, \n",
    "                                trainLoader, testLoader, device, \n",
    "                                num_epochs=CONFIGS['NUM_EPOCHS'], \n",
    "                                early_stopping_patience=CONFIGS['EARLY_STOPPING_PATIENCE'])\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "print(\"Model training completed...\")\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time spent: {round(execution_time/60,2)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7c51679-7fe1-4459-b105-9692bf06a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving label encoder...\n"
     ]
    }
   ],
   "source": [
    "torch.save(model_ft.state_dict(), 'output/multi_head_model.pth')\n",
    "\n",
    "print(\"[INFO] saving label encoder...\")\n",
    "f = open(CONFIGS[\"LE_PATH_PRDTYPE\"], \"wb\")\n",
    "f.write(pickle.dumps(le_prdtype))\n",
    "f.close()\n",
    "# f = open(CONFIGS[\"LE_PATH_WEIGHT\"], \"wb\")\n",
    "# f.write(pickle.dumps(le_weight))\n",
    "# f.close()\n",
    "# f = open(CONFIGS[\"LE_PATH_HALAL\"], \"wb\")\n",
    "# f.write(pickle.dumps(le_halal))\n",
    "# f.close()\n",
    "# f = open(CONFIGS[\"LE_PATH_HEALTHY\"], \"wb\")\n",
    "# f.write(pickle.dumps(le_healthy))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de520c9-41e2-4c11-bed9-67caf2609523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracies: {'ProductType': 0.9725609756097561}\n",
      "Test Accuracies: {'ProductType': 0.6097560975609756}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, dataset_size, num_mc_samples=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_counts = {'ProductType': 0}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, labels_prdtype, filenames) in data_loader:\n",
    "            images = images.to(CONFIGS['DEVICE'])\n",
    "            labels_prdtype = labels_prdtype.to(CONFIGS['DEVICE'])\n",
    "\n",
    "            # Forward pass\n",
    "            out1 = model(images)\n",
    "\n",
    "            # Store deterministic predictions\n",
    "            det_pred_prdtype = out1.argmax(1)\n",
    "\n",
    "            # Update correct counts for each category\n",
    "            correct_counts['ProductType'] += (out1.argmax(1) == labels_prdtype).float().sum().item()\n",
    "\n",
    "    # Calculate accuracies\n",
    "    accuracies = {key: correct_counts[key] / dataset_size for key in correct_counts}\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracies = evaluate_model(model_ft, trainLoader, len(trainDS))\n",
    "print(f\"Training Accuracies: {train_accuracies}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracies= evaluate_model(model_ft, testLoader, len(testDS))\n",
    "print(f\"Test Accuracies: {test_accuracies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ca685c-9918-42e2-b8eb-e99179fda09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on new images\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "all_filepaths, all_prdlabel_preds = [], []\n",
    "\n",
    "for i in range(len(annotations_2)):\n",
    "    image_path = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], annotations_2['filepath'][i])\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    # Preprocessing steps\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (CONFIGS['SMALL_MODEL_IMG_SIZE'], CONFIGS['SMALL_MODEL_IMG_SIZE']))\n",
    "    frame = frame.transpose((2, 0, 1))\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    frame = test_transforms(frame).unsqueeze(0).to(CONFIGS['DEVICE'])\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        out1 = model_ft(frame)\n",
    "    \n",
    "    # Extract and store the results\n",
    "    tmp_pred = out1.argmax(1)\n",
    "    tmp_pred_label = le_prdtype.inverse_transform([tmp_pred])\n",
    "    all_prdlabel_preds.append(tmp_pred_label[0])\n",
    "\n",
    "\n",
    "print(\"Accuracy on new images\")\n",
    "print(sum(all_prdlabel_preds == annotations_2['ProductType']) / len(annotations_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e78a4cd-8cf4-455a-b6cd-65284d08908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410, 15)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a319c9f-c8b2-4cef-9a04-88699c5a1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = []\n",
    "\n",
    "for i in range(len(annotations)):\n",
    "    image_path = os.path.join(CONFIGS[\"DATA_BASE_PATH\"], annotations['filepath'][i])\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    # Preprocessing steps\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (CONFIGS['SMALL_MODEL_IMG_SIZE'], CONFIGS['SMALL_MODEL_IMG_SIZE']))\n",
    "    frame = frame.transpose((2, 0, 1))\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    frame = test_transforms(frame).unsqueeze(0).to(CONFIGS['DEVICE'])\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        out1 = model_ft(frame)\n",
    "    \n",
    "    # Extract and store the results\n",
    "    all_logits.append(out1[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea530d84-aeed-4030-8e64-a552c33bc1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>label</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HalalStatus</th>\n",
       "      <th>HealthStatus</th>\n",
       "      <th>...</th>\n",
       "      <th>Potatochips</th>\n",
       "      <th>RiceBrownOthers</th>\n",
       "      <th>RiceWhite</th>\n",
       "      <th>RolledOatsInstantOatmeal</th>\n",
       "      <th>Salt</th>\n",
       "      <th>SoftDrinksOtherReadyToDrink</th>\n",
       "      <th>SoupStock</th>\n",
       "      <th>Sugar</th>\n",
       "      <th>SweetsChocolatesOthers</th>\n",
       "      <th>TeaPowderLeaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023_10_25_12_0_34_784872.jpg</td>\n",
       "      <td>285</td>\n",
       "      <td>168</td>\n",
       "      <td>567</td>\n",
       "      <td>528</td>\n",
       "      <td>BabyMilkPowder_200-299g_Halal_NonHealthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>200-299g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.538441</td>\n",
       "      <td>-2.474456</td>\n",
       "      <td>-2.166450</td>\n",
       "      <td>-2.289746</td>\n",
       "      <td>-1.916843</td>\n",
       "      <td>-0.356248</td>\n",
       "      <td>-1.462294</td>\n",
       "      <td>-1.907547</td>\n",
       "      <td>-1.365697</td>\n",
       "      <td>-2.333017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023_10_25_11_50_16_569076.jpg</td>\n",
       "      <td>266</td>\n",
       "      <td>207</td>\n",
       "      <td>506</td>\n",
       "      <td>528</td>\n",
       "      <td>BabyMilkPowder_400-499g_Halal_NonHealthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>400-499g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542177</td>\n",
       "      <td>-2.324751</td>\n",
       "      <td>-1.815467</td>\n",
       "      <td>-2.294456</td>\n",
       "      <td>-1.651892</td>\n",
       "      <td>-1.226976</td>\n",
       "      <td>-2.332330</td>\n",
       "      <td>0.209407</td>\n",
       "      <td>-1.838781</td>\n",
       "      <td>-2.299150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023_10_25_11_45_37_343791.jpg</td>\n",
       "      <td>230</td>\n",
       "      <td>80</td>\n",
       "      <td>562</td>\n",
       "      <td>528</td>\n",
       "      <td>BabyMilkPowder_800-899g_Halal_Healthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>800-899g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371769</td>\n",
       "      <td>-1.519539</td>\n",
       "      <td>-0.456094</td>\n",
       "      <td>0.085578</td>\n",
       "      <td>-2.225759</td>\n",
       "      <td>-1.688356</td>\n",
       "      <td>-2.762919</td>\n",
       "      <td>-0.552690</td>\n",
       "      <td>-0.507280</td>\n",
       "      <td>-1.503624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023_10_25_12_9_12_144686.jpg</td>\n",
       "      <td>200</td>\n",
       "      <td>140</td>\n",
       "      <td>532</td>\n",
       "      <td>528</td>\n",
       "      <td>BabyMilkPowder_900-999g_NonHalal_NonHealthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>900-999g</td>\n",
       "      <td>NonHalal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>-2.333671</td>\n",
       "      <td>-1.224976</td>\n",
       "      <td>-2.133831</td>\n",
       "      <td>-0.818596</td>\n",
       "      <td>-1.456577</td>\n",
       "      <td>-2.075854</td>\n",
       "      <td>0.011306</td>\n",
       "      <td>-1.141832</td>\n",
       "      <td>-1.817970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023_10_25_12_8_11_460440.jpg</td>\n",
       "      <td>218</td>\n",
       "      <td>125</td>\n",
       "      <td>536</td>\n",
       "      <td>528</td>\n",
       "      <td>BabyMilkPowder_800-899g_Halal_NonHealthy</td>\n",
       "      <td>BabyMilkPowder</td>\n",
       "      <td>800-899g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.578252</td>\n",
       "      <td>-2.372956</td>\n",
       "      <td>-2.174664</td>\n",
       "      <td>-1.919113</td>\n",
       "      <td>-2.649999</td>\n",
       "      <td>-0.679314</td>\n",
       "      <td>-1.787639</td>\n",
       "      <td>-0.066358</td>\n",
       "      <td>-0.924837</td>\n",
       "      <td>-2.095980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filepath  xmin  ymin  xmax  ymax  \\\n",
       "0   2023_10_25_12_0_34_784872.jpg   285   168   567   528   \n",
       "1  2023_10_25_11_50_16_569076.jpg   266   207   506   528   \n",
       "2  2023_10_25_11_45_37_343791.jpg   230    80   562   528   \n",
       "3   2023_10_25_12_9_12_144686.jpg   200   140   532   528   \n",
       "4   2023_10_25_12_8_11_460440.jpg   218   125   536   528   \n",
       "\n",
       "                                         label     ProductType    Weight  \\\n",
       "0     BabyMilkPowder_200-299g_Halal_NonHealthy  BabyMilkPowder  200-299g   \n",
       "1     BabyMilkPowder_400-499g_Halal_NonHealthy  BabyMilkPowder  400-499g   \n",
       "2        BabyMilkPowder_800-899g_Halal_Healthy  BabyMilkPowder  800-899g   \n",
       "3  BabyMilkPowder_900-999g_NonHalal_NonHealthy  BabyMilkPowder  900-999g   \n",
       "4     BabyMilkPowder_800-899g_Halal_NonHealthy  BabyMilkPowder  800-899g   \n",
       "\n",
       "  HalalStatus HealthStatus  ...  Potatochips RiceBrownOthers  RiceWhite  \\\n",
       "0       Halal   NonHealthy  ...    -1.538441       -2.474456  -2.166450   \n",
       "1       Halal   NonHealthy  ...    -0.542177       -2.324751  -1.815467   \n",
       "2       Halal      Healthy  ...     1.371769       -1.519539  -0.456094   \n",
       "3    NonHalal   NonHealthy  ...     0.002006       -2.333671  -1.224976   \n",
       "4       Halal   NonHealthy  ...    -0.578252       -2.372956  -2.174664   \n",
       "\n",
       "  RolledOatsInstantOatmeal      Salt SoftDrinksOtherReadyToDrink  SoupStock  \\\n",
       "0                -2.289746 -1.916843                   -0.356248  -1.462294   \n",
       "1                -2.294456 -1.651892                   -1.226976  -2.332330   \n",
       "2                 0.085578 -2.225759                   -1.688356  -2.762919   \n",
       "3                -2.133831 -0.818596                   -1.456577  -2.075854   \n",
       "4                -1.919113 -2.649999                   -0.679314  -1.787639   \n",
       "\n",
       "      Sugar  SweetsChocolatesOthers  TeaPowderLeaves  \n",
       "0 -1.907547               -1.365697        -2.333017  \n",
       "1  0.209407               -1.838781        -2.299150  \n",
       "2 -0.552690               -0.507280        -1.503624  \n",
       "3  0.011306               -1.141832        -1.817970  \n",
       "4 -0.066358               -0.924837        -2.095980  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert list of lists to DataFrame\n",
    "all_logits_df = pd.DataFrame(all_logits)\n",
    "all_logits_df.columns = le_prdtype.classes_\n",
    "all_logits_df['pred_prdtype'] = all_logits_df.idxmax(axis=1)\n",
    "target_column = 'pred_prdtype'\n",
    "columns = [target_column] + [col for col in all_logits_df.columns if col != target_column]\n",
    "all_logits_df = all_logits_df[columns]\n",
    "\n",
    "final_df = pd.concat([annotations, all_logits_df], axis=1)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e5d3c44-14e7-4806-9dab-febf507eebc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9219512195121952"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_df['ProductType'] == final_df['pred_prdtype']) / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e3d34e3-942f-4579-93e5-5f912149d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('new_imgs_results_small_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7002b9e-4c36-4cb9-abd1-dd64145b33fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filepath', 'xmin', 'ymin', 'xmax', 'ymax', 'label', 'ProductType',\n",
       "       'Weight', 'HalalStatus', 'HealthStatus', 'new_camera', 'tag', 'remove',\n",
       "       'tokeep', 'type', 'pred_prdtype', 'BabyMilkPowder', 'Babyfood',\n",
       "       'BeehoonVermicelli', 'BiscuitsCrackersCookies', 'BreakfastCereals',\n",
       "       'CannedBakedBeans', 'CannedBeefOtherMeats', 'CannedBraisedPeanuts',\n",
       "       'CannedChicken', 'CannedFruits', 'CannedMushrooms',\n",
       "       'CannedPacketCreamersSweet', 'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink', 'Flour',\n",
       "       'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood', 'OtherHotBeveragesPowder',\n",
       "       'OtherNoodles', 'OtherSauceDressing', 'OtherSpreads', 'Pasta',\n",
       "       'PastaSauce', 'PeanutButter', 'Potatochips', 'RiceBrownOthers',\n",
       "       'RiceWhite', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f598c285-4e6d-48ac-8496-7e87429fdc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BabyMilkPowder', 'Babyfood', 'BeehoonVermicelli',\n",
       "       'BiscuitsCrackersCookies', 'BreakfastCereals', 'CannedBakedBeans',\n",
       "       'CannedBeefOtherMeats', 'CannedBraisedPeanuts', 'CannedChicken',\n",
       "       'CannedFruits', 'CannedMushrooms', 'CannedPacketCreamersSweet',\n",
       "       'CannedPickles', 'CannedPorkLunchronMeat',\n",
       "       'CannedSardinesMackerel', 'CannedSoup', 'CannedTunaDace',\n",
       "       'CannedVegetarianFood', 'ChocolateMaltPowder', 'ChocolateSpread',\n",
       "       'CoffeePowder', 'CoffeeTeaDrink', 'CookingCreamMilk',\n",
       "       'CookingPastePowder', 'DarkSoySauce', 'DriedBeans', 'DriedFruits',\n",
       "       'DriedMeatSeafood', 'DriedVegetables', 'FlavoredMilkDrink',\n",
       "       'Flour', 'FruitJuiceDrink', 'HerbsSpices', 'InstantMeals',\n",
       "       'InstantNoodlesMultipack', 'InstantNoodlesSingle', 'Jam', 'Kaya',\n",
       "       'KetchupChilliSauce', 'LightSoySauce', 'MaternalMilkPowder',\n",
       "       'MilkDrink', 'MilkPowder', 'Nuts', 'Oil', 'OtherBakingNeeds',\n",
       "       'OtherCannedBeansPeasNuts', 'OtherCannedSeafood',\n",
       "       'OtherCannedVegetables', 'OtherDriedFood',\n",
       "       'OtherHotBeveragesPowder', 'OtherNoodles', 'OtherSauceDressing',\n",
       "       'OtherSpreads', 'Pasta', 'PastaSauce', 'PeanutButter',\n",
       "       'Potatochips', 'RolledOatsInstantOatmeal', 'Salt',\n",
       "       'SoftDrinksOtherReadyToDrink', 'SoupStock', 'Sugar',\n",
       "       'SweetsChocolatesOthers', 'TeaPowderLeaves', 'RiceBrownOthers',\n",
       "       'RiceWhite'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['ProductType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72706b-9da3-4cc9-8da6-1c6ace5df235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
