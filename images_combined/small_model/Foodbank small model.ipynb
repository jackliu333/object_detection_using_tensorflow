{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0aa7ae9-f9cd-492f-8f87-5e0c63ef709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import copy\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed7bde29-0d3e-4cc2-b0a4-1f1dcdb63807",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # specify ImageNet mean and standard deviation\n",
    "    \"IMG_MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"IMG_STD\": [0.229, 0.224, 0.225],\n",
    "    \"INIT_LR\": 1e-4,\n",
    "    \"NUM_EPOCHS\": 200,\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    # specify the loss weights\n",
    "    \"LABELS_PRDTYPE\": 1.0,\n",
    "    \"LABELS_WEIGHT\": 1.0,\n",
    "    \"LABELS_HALAL\": 1.0,\n",
    "    \"LABELS_HEALTHY\": 1.0,\n",
    "    \"MODEL_PATH\": os.path.sep.join([\"output\", \"detector.pth\"]),\n",
    "    \"LE_PATH_PRDTYPE\": os.path.sep.join([\"output\", \"le_prdtype.pickle\"]),\n",
    "    \"LE_PATH_WEIGHT\": os.path.sep.join([\"output\", \"le_weight.pickle\"]),\n",
    "    \"LE_PATH_HALAL\": os.path.sep.join([\"output\", \"le_halal.pickle\"]),\n",
    "    \"LE_PATH_HEALTHY\": os.path.sep.join([\"output\", \"le_healthy.pickle\"]),\n",
    "    \"PIN_MEMORY\": True if torch.cuda.is_available() else False,\n",
    "    \"DATA_BASE_PATH\": \"/Users/liupeng/Documents/GitHub/object_detection_using_tensorflow/images_combined/all_images\",\n",
    "    \"NEW_DATA_BASE_PATH\": \"/Users/liupeng/Documents/GitHub/object_detection_using_tensorflow/images_combined/small_model/new_imgs\"\n",
    "}\n",
    "\n",
    "# create output folder\n",
    "if not os.path.exists(\"output\"):\n",
    "    !mkdir -p {\"output\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d80f47-b6ca-4064-9752-40c14fbcbd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>label</th>\n",
       "      <th>ProductType</th>\n",
       "      <th>Weight</th>\n",
       "      <th>HalalStatus</th>\n",
       "      <th>HealthStatus</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023_10_25_11_30_29_761991.jpg</td>\n",
       "      <td>AdultMilk_1-99g_Halal_NonHealthy</td>\n",
       "      <td>AdultMilk</td>\n",
       "      <td>1-99g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023_10_25_11_23_46_148599.jpg</td>\n",
       "      <td>AdultMilk_1-99g_Halal_NonHealthy</td>\n",
       "      <td>AdultMilk</td>\n",
       "      <td>1-99g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023_10_25_11_29_17_748204.jpg</td>\n",
       "      <td>AdultMilk_1-99g_Halal_NonHealthy</td>\n",
       "      <td>AdultMilk</td>\n",
       "      <td>1-99g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023_10_25_11_29_34_492008.jpg</td>\n",
       "      <td>AdultMilk_1-99g_Halal_NonHealthy</td>\n",
       "      <td>AdultMilk</td>\n",
       "      <td>1-99g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023_10_25_11_27_5_184489.jpg</td>\n",
       "      <td>AdultMilk_1000-1999g_Halal_NonHealthy</td>\n",
       "      <td>AdultMilk</td>\n",
       "      <td>1000-1999g</td>\n",
       "      <td>Halal</td>\n",
       "      <td>NonHealthy</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filepath                                  label  \\\n",
       "0  2023_10_25_11_30_29_761991.jpg       AdultMilk_1-99g_Halal_NonHealthy   \n",
       "1  2023_10_25_11_23_46_148599.jpg       AdultMilk_1-99g_Halal_NonHealthy   \n",
       "2  2023_10_25_11_29_17_748204.jpg       AdultMilk_1-99g_Halal_NonHealthy   \n",
       "3  2023_10_25_11_29_34_492008.jpg       AdultMilk_1-99g_Halal_NonHealthy   \n",
       "4   2023_10_25_11_27_5_184489.jpg  AdultMilk_1000-1999g_Halal_NonHealthy   \n",
       "\n",
       "  ProductType      Weight HalalStatus HealthStatus Type  \n",
       "0   AdultMilk       1-99g       Halal   NonHealthy  old  \n",
       "1   AdultMilk       1-99g       Halal   NonHealthy  old  \n",
       "2   AdultMilk       1-99g       Halal   NonHealthy  old  \n",
       "3   AdultMilk       1-99g       Halal   NonHealthy  old  \n",
       "4   AdultMilk  1000-1999g       Halal   NonHealthy  old  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # ten classes from existing imgs\n",
    "# annotations_1 = pd.read_csv(\"base_imgs_list.csv\")\n",
    "# annotations_1.reset_index(drop=True, inplace=True)\n",
    "# annotations_1['Type'] = \"old\"\n",
    "\n",
    "# all classes from existing imgs\n",
    "annotations_1 = pd.read_csv(\"../master_list.csv\")\n",
    "annotations_1 = annotations_1.groupby('label', group_keys=False).apply(lambda x: x.sample(min(len(x), 4)))\n",
    "annotations_1 = annotations_1[[\"filepath\", \"label\",\t\"ProductType\", \"Weight\", \"HalalStatus\",\t\"HealthStatus\"]]\n",
    "annotations_1.reset_index(drop=True, inplace=True)\n",
    "annotations_1['Type'] = \"old\"\n",
    "\n",
    "annotations_2 = pd.read_csv(\"new_imgs_list.csv\")\n",
    "annotations_2.reset_index(drop=True, inplace=True)\n",
    "annotations_2['Type'] = \"new\"\n",
    "\n",
    "# ADHOC: change the new imgs to existing type\n",
    "annotations_2['label'] = 'AdultMilk_1-99g_Halal_NonHealthy'\n",
    "annotations_2['ProductType'] = 'AdultMilk'\n",
    "annotations_2['Weight'] = '1-99g'\n",
    "annotations_2['HalalStatus'] = 'Halal'\n",
    "annotations_2['HealthStatus'] = 'NonHealthy'\n",
    "\n",
    "# Concatenate the two dataframes vertically\n",
    "annotations = pd.concat([annotations_1, annotations_2], ignore_index=True)\n",
    "annotations.reset_index(drop=True, inplace=True)\n",
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2676357-d825-4673-8a99-bb919b2409e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for processed data\n",
    "data, imagePaths, filenames = [], [], []\n",
    "\n",
    "# Process each annotation entry\n",
    "for idx, row in annotations.iterrows():\n",
    "    filepath = row[\"filepath\"]\n",
    "    if row['Type'] == 'old':\n",
    "        imagePath = os.path.join(\"/content\", CONFIGS[\"DATA_BASE_PATH\"], filepath)\n",
    "    else:\n",
    "        imagePath = os.path.join(\"/content\", CONFIGS[\"NEW_DATA_BASE_PATH\"], filepath)\n",
    "    image = cv2.imread(imagePath)\n",
    "    \n",
    "    # Preprocess image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (60, 60))\n",
    "\n",
    "    # Append processed data to lists\n",
    "    data.append(image)\n",
    "    imagePaths.append(imagePath)\n",
    "    # filenames.append(filepath.rsplit('.', 1)[0])\n",
    "    filenames.append(filepath)\n",
    "\n",
    "# Convert data to NumPy arrays for machine learning processing\n",
    "labels = {\n",
    "    'labels_prdtype': annotations['ProductType'],\n",
    "    'labels_weight': annotations['Weight'],\n",
    "    'labels_halal': annotations['HalalStatus'],\n",
    "    'labels_healthy': annotations['HealthStatus'],\n",
    "    'labels_total': annotations['label']\n",
    "}\n",
    "\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "for label_name, label_data in labels.items():\n",
    "    labels[label_name] = np.array(label_data)\n",
    "\n",
    "# Split the data and labels into training and testing sets\n",
    "split = train_test_split(data, *labels.values(), imagePaths, filenames,\n",
    "                         test_size=0.3, random_state=42, stratify=labels['labels_total'])\n",
    "\n",
    "# Unpack the data split\n",
    "(trainImages, testImages, *split_labels, trainPaths, testPaths, trainFilenames, testFilenames) = split\n",
    "\n",
    "# Create label encoders and transform labels\n",
    "le_prdtype = LabelEncoder()\n",
    "le_weight = LabelEncoder()\n",
    "le_halal = LabelEncoder()\n",
    "le_healthy = LabelEncoder()\n",
    "le_total = LabelEncoder()\n",
    "\n",
    "trainLabels = {}\n",
    "testLabels = {}\n",
    "\n",
    "# Fit label encoders and transform labels\n",
    "trainLabels['labels_prdtype'] = le_prdtype.fit_transform(split_labels[0])\n",
    "testLabels['labels_prdtype'] = le_prdtype.transform(split_labels[1])\n",
    "\n",
    "trainLabels['labels_weight'] = le_weight.fit_transform(split_labels[2])\n",
    "testLabels['labels_weight'] = le_weight.transform(split_labels[3])\n",
    "\n",
    "trainLabels['labels_halal'] = le_halal.fit_transform(split_labels[4])\n",
    "testLabels['labels_halal'] = le_halal.transform(split_labels[5])\n",
    "\n",
    "trainLabels['labels_healthy'] = le_healthy.fit_transform(split_labels[6])\n",
    "testLabels['labels_healthy'] = le_healthy.transform(split_labels[7])\n",
    "\n",
    "trainLabels['labels_total'] = le_total.fit_transform(split_labels[8])\n",
    "testLabels['labels_total'] = le_total.transform(split_labels[9])\n",
    "\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "trainImages, testImages = torch.tensor(trainImages), torch.tensor(testImages)\n",
    "for label_name in labels.keys():\n",
    "    trainLabels[label_name] = torch.tensor(trainLabels[label_name])\n",
    "    testLabels[label_name] = torch.tensor(testLabels[label_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e5c38e-bc8c-4eb5-84c3-61fffecca02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8b870a-5fab-4f78-8559-b428f64cd1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] total training samples: 470...\n",
      "[INFO] total test samples: 202...\n"
     ]
    }
   ],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    # Initialize the constructor\n",
    "    def __init__(self, images, labels, filenames, transforms=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.filenames = filenames\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Grab the image, labels, and its bounding box coordinates\n",
    "        image = self.images[index]\n",
    "        label_prdtype = self.labels['labels_prdtype'][index]\n",
    "        label_weight = self.labels['labels_weight'][index]\n",
    "        label_halal = self.labels['labels_halal'][index]\n",
    "        label_healthy = self.labels['labels_healthy'][index]\n",
    "        filename = self.filenames[index]\n",
    "\n",
    "        # Transpose the image such that its channel dimension becomes the leading one\n",
    "        image = image.permute(2, 0, 1)\n",
    "\n",
    "        # Check to see if we have any image transformations to apply and if so, apply them\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # Return a tuple of the images, labels, and bounding box coordinates\n",
    "        return (image, label_prdtype, label_weight, label_halal, label_healthy, filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.images)\n",
    "\n",
    "# Define normalization and augmentation transforms\n",
    "normalization_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIGS['IMG_MEAN'], std=CONFIGS['IMG_STD'])\n",
    "])\n",
    "\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(20),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n",
    "])\n",
    "\n",
    "# Combine augmentation and normalization for training\n",
    "train_transforms = transforms.Compose([augmentation_transforms, normalization_transforms])\n",
    "test_transforms = normalization_transforms\n",
    "\n",
    "# Create PyTorch datasets\n",
    "trainDS = CustomTensorDataset(trainImages, trainLabels, trainFilenames, transforms=train_transforms)\n",
    "testDS = CustomTensorDataset(testImages, testLabels, testFilenames, transforms=test_transforms)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"[INFO] total training samples: {}...\".format(len(trainDS)))\n",
    "print(\"[INFO] total test samples: {}...\".format(len(testDS)))\n",
    "\n",
    "# Calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDS) // CONFIGS['BATCH_SIZE']\n",
    "valSteps = len(testDS) // CONFIGS['BATCH_SIZE']\n",
    "\n",
    "# Create data loaders\n",
    "trainLoader = DataLoader(trainDS, batch_size=CONFIGS['BATCH_SIZE'], shuffle=True,\n",
    "                         num_workers=os.cpu_count(), pin_memory=CONFIGS['PIN_MEMORY'])\n",
    "testLoader = DataLoader(testDS, batch_size=CONFIGS['BATCH_SIZE'],\n",
    "                        num_workers=os.cpu_count(), pin_memory=CONFIGS['PIN_MEMORY'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08cf52af-29f1-4ed4-bc62-24673572e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liupeng/opt/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/Users/liupeng/opt/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training started...\n",
      "Epoch 1/200\n",
      "train Loss: 7.7592 Acc: 0.0000\n",
      "epochs_no_improve: 0\n",
      "val Loss: 7.0509 Acc: 0.0050\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 2/200\n",
      "train Loss: 6.3348 Acc: 0.0298\n",
      "epochs_no_improve: 0\n",
      "val Loss: 6.3826 Acc: 0.0248\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 3/200\n",
      "train Loss: 5.5570 Acc: 0.0894\n",
      "epochs_no_improve: 0\n",
      "val Loss: 5.8533 Acc: 0.0644\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 4/200\n",
      "train Loss: 5.0103 Acc: 0.1191\n",
      "epochs_no_improve: 0\n",
      "val Loss: 5.5080 Acc: 0.1287\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 5/200\n",
      "train Loss: 4.6523 Acc: 0.1660\n",
      "epochs_no_improve: 0\n",
      "val Loss: 5.0240 Acc: 0.1535\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 6/200\n",
      "train Loss: 4.0893 Acc: 0.2383\n",
      "epochs_no_improve: 0\n",
      "val Loss: 4.7961 Acc: 0.2079\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 7/200\n",
      "train Loss: 3.8027 Acc: 0.2681\n",
      "epochs_no_improve: 0\n",
      "val Loss: 4.5433 Acc: 0.2030\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 8/200\n",
      "train Loss: 3.4287 Acc: 0.3340\n",
      "epochs_no_improve: 0\n",
      "val Loss: 4.3812 Acc: 0.2574\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 9/200\n",
      "train Loss: 3.1735 Acc: 0.4128\n",
      "epochs_no_improve: 0\n",
      "val Loss: 4.2413 Acc: 0.2871\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 10/200\n",
      "train Loss: 2.8023 Acc: 0.4553\n",
      "epochs_no_improve: 0\n",
      "val Loss: 4.0668 Acc: 0.2970\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 11/200\n",
      "train Loss: 2.4523 Acc: 0.5383\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.9540 Acc: 0.3317\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 12/200\n",
      "train Loss: 2.3353 Acc: 0.5894\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.8568 Acc: 0.3317\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 13/200\n",
      "train Loss: 2.1211 Acc: 0.5872\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.7388 Acc: 0.3762\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 14/200\n",
      "train Loss: 1.9310 Acc: 0.6362\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.7447 Acc: 0.3515\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 15/200\n",
      "train Loss: 1.8347 Acc: 0.6872\n",
      "epochs_no_improve: 1\n",
      "val Loss: 3.6227 Acc: 0.3812\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 16/200\n",
      "train Loss: 1.5795 Acc: 0.7191\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.6398 Acc: 0.3960\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 17/200\n",
      "train Loss: 1.5800 Acc: 0.6979\n",
      "epochs_no_improve: 1\n",
      "val Loss: 3.6017 Acc: 0.3812\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 18/200\n",
      "train Loss: 1.4276 Acc: 0.7319\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.5088 Acc: 0.4208\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 19/200\n",
      "train Loss: 1.2810 Acc: 0.7596\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.4851 Acc: 0.4356\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 20/200\n",
      "train Loss: 1.2744 Acc: 0.7383\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.3822 Acc: 0.4158\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 21/200\n",
      "train Loss: 1.0961 Acc: 0.8085\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.3599 Acc: 0.4208\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 22/200\n",
      "train Loss: 1.0597 Acc: 0.8085\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.5076 Acc: 0.4406\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 23/200\n",
      "train Loss: 0.9582 Acc: 0.8447\n",
      "epochs_no_improve: 1\n",
      "val Loss: 3.5010 Acc: 0.4505\n",
      "tmp epochs_no_improve: 2\n",
      "epochs_no_improve: 2\n",
      "Epoch 24/200\n",
      "train Loss: 1.0052 Acc: 0.8340\n",
      "epochs_no_improve: 2\n",
      "val Loss: 3.4756 Acc: 0.4455\n",
      "tmp epochs_no_improve: 3\n",
      "epochs_no_improve: 3\n",
      "Epoch 25/200\n",
      "train Loss: 0.9689 Acc: 0.8149\n",
      "epochs_no_improve: 3\n",
      "val Loss: 3.2629 Acc: 0.4950\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 26/200\n",
      "train Loss: 0.8959 Acc: 0.8213\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.2789 Acc: 0.5099\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 27/200\n",
      "train Loss: 0.8310 Acc: 0.8383\n",
      "epochs_no_improve: 1\n",
      "val Loss: 3.3325 Acc: 0.4901\n",
      "tmp epochs_no_improve: 2\n",
      "epochs_no_improve: 2\n",
      "Epoch 28/200\n",
      "train Loss: 0.7279 Acc: 0.8787\n",
      "epochs_no_improve: 2\n",
      "val Loss: 3.3121 Acc: 0.4950\n",
      "tmp epochs_no_improve: 3\n",
      "epochs_no_improve: 3\n",
      "Epoch 29/200\n",
      "train Loss: 0.7355 Acc: 0.8745\n",
      "epochs_no_improve: 3\n",
      "val Loss: 3.3564 Acc: 0.4802\n",
      "tmp epochs_no_improve: 4\n",
      "epochs_no_improve: 4\n",
      "Epoch 30/200\n",
      "train Loss: 0.6077 Acc: 0.8936\n",
      "epochs_no_improve: 4\n",
      "val Loss: 3.2063 Acc: 0.5099\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 31/200\n",
      "train Loss: 0.6723 Acc: 0.8617\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.2845 Acc: 0.5198\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 32/200\n",
      "train Loss: 0.6350 Acc: 0.8894\n",
      "epochs_no_improve: 1\n",
      "val Loss: 3.2841 Acc: 0.4802\n",
      "tmp epochs_no_improve: 2\n",
      "epochs_no_improve: 2\n",
      "Epoch 33/200\n",
      "train Loss: 0.6319 Acc: 0.8915\n",
      "epochs_no_improve: 2\n",
      "val Loss: 3.4520 Acc: 0.4802\n",
      "tmp epochs_no_improve: 3\n",
      "epochs_no_improve: 3\n",
      "Epoch 34/200\n",
      "train Loss: 0.5637 Acc: 0.8851\n",
      "epochs_no_improve: 3\n",
      "val Loss: 3.3345 Acc: 0.4703\n",
      "tmp epochs_no_improve: 4\n",
      "epochs_no_improve: 4\n",
      "Epoch 35/200\n",
      "train Loss: 0.4953 Acc: 0.9106\n",
      "epochs_no_improve: 4\n",
      "val Loss: 3.3466 Acc: 0.4901\n",
      "tmp epochs_no_improve: 5\n",
      "epochs_no_improve: 5\n",
      "Epoch 36/200\n",
      "train Loss: 0.5787 Acc: 0.8872\n",
      "epochs_no_improve: 5\n",
      "val Loss: 3.3325 Acc: 0.5050\n",
      "tmp epochs_no_improve: 6\n",
      "epochs_no_improve: 6\n",
      "Epoch 37/200\n",
      "train Loss: 0.5402 Acc: 0.8979\n",
      "epochs_no_improve: 6\n",
      "val Loss: 3.3050 Acc: 0.4950\n",
      "tmp epochs_no_improve: 7\n",
      "epochs_no_improve: 7\n",
      "Epoch 38/200\n",
      "train Loss: 0.5037 Acc: 0.8936\n",
      "epochs_no_improve: 7\n",
      "val Loss: 3.1923 Acc: 0.5149\n",
      "new loss obtained\n",
      "epochs_no_improve: 0\n",
      "Epoch 39/200\n",
      "train Loss: 0.5458 Acc: 0.8936\n",
      "epochs_no_improve: 0\n",
      "val Loss: 3.3917 Acc: 0.5050\n",
      "tmp epochs_no_improve: 1\n",
      "epochs_no_improve: 1\n",
      "Epoch 40/200\n",
      "train Loss: 0.5188 Acc: 0.9043\n",
      "epochs_no_improve: 1\n",
      "val Loss: 3.4293 Acc: 0.4901\n",
      "tmp epochs_no_improve: 2\n",
      "epochs_no_improve: 2\n",
      "Epoch 41/200\n",
      "train Loss: 0.3935 Acc: 0.9319\n",
      "epochs_no_improve: 2\n",
      "val Loss: 3.3726 Acc: 0.5198\n",
      "tmp epochs_no_improve: 3\n",
      "epochs_no_improve: 3\n",
      "Epoch 42/200\n",
      "train Loss: 0.4528 Acc: 0.9191\n",
      "epochs_no_improve: 3\n",
      "val Loss: 3.3507 Acc: 0.5545\n",
      "tmp epochs_no_improve: 4\n",
      "epochs_no_improve: 4\n",
      "Epoch 43/200\n",
      "train Loss: 0.4748 Acc: 0.9149\n",
      "epochs_no_improve: 4\n",
      "val Loss: 3.2769 Acc: 0.5099\n",
      "tmp epochs_no_improve: 5\n",
      "epochs_no_improve: 5\n",
      "Epoch 44/200\n",
      "train Loss: 0.4266 Acc: 0.9213\n",
      "epochs_no_improve: 5\n",
      "val Loss: 3.4280 Acc: 0.5347\n",
      "tmp epochs_no_improve: 6\n",
      "epochs_no_improve: 6\n",
      "Epoch 45/200\n",
      "train Loss: 0.4332 Acc: 0.9191\n",
      "epochs_no_improve: 6\n",
      "val Loss: 3.4697 Acc: 0.5149\n",
      "tmp epochs_no_improve: 7\n",
      "epochs_no_improve: 7\n",
      "Epoch 46/200\n",
      "train Loss: 0.3983 Acc: 0.9128\n",
      "epochs_no_improve: 7\n",
      "val Loss: 3.3924 Acc: 0.5248\n",
      "tmp epochs_no_improve: 8\n",
      "epochs_no_improve: 8\n",
      "Epoch 47/200\n",
      "train Loss: 0.5129 Acc: 0.8936\n",
      "epochs_no_improve: 8\n",
      "val Loss: 3.4047 Acc: 0.5347\n",
      "tmp epochs_no_improve: 9\n",
      "epochs_no_improve: 9\n",
      "Epoch 48/200\n",
      "train Loss: 0.3856 Acc: 0.9277\n",
      "epochs_no_improve: 9\n",
      "val Loss: 3.4793 Acc: 0.4802\n",
      "tmp epochs_no_improve: 10\n",
      "Early stopping triggered at epoch: 48\n",
      "Model training completed...\n",
      "Time spent: 13.97 mins\n"
     ]
    }
   ],
   "source": [
    "# Define the MultiHeadResNet model\n",
    "class MultiHeadResNet(nn.Module):\n",
    "    def __init__(self, num_classes_prdtype, num_classes_weight, num_classes_halal, num_classes_healthy):\n",
    "        super(MultiHeadResNet, self).__init__()\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "        # Define custom fully connected layers for each prediction head\n",
    "        self.fc_prdtype = nn.Linear(num_ftrs, num_classes_prdtype)\n",
    "        self.fc_weight = nn.Linear(num_ftrs, num_classes_weight)\n",
    "        self.fc_halal = nn.Linear(num_ftrs, num_classes_halal)\n",
    "        self.fc_healthy = nn.Linear(num_ftrs, num_classes_healthy)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        prdtype = self.fc_prdtype(x)\n",
    "        weight = self.fc_weight(x)\n",
    "        halal = self.fc_halal(x)\n",
    "        healthy = self.fc_healthy(x)\n",
    "        return prdtype, weight, halal, healthy\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    corrects = torch.sum(preds == labels.data)\n",
    "    return corrects.double() / labels.size(0)\n",
    "\n",
    "# Training and Validation Loop with Early Stopping\n",
    "def train_model(model, criteria, optimizer, train_loader, test_loader, device, num_epochs=25, early_stopping_patience=10):\n",
    "    criterion_prdtype, criterion_weight, criterion_halal, criterion_healthy = criteria\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc_prdtype': [],\n",
    "        'train_acc_weight': [],\n",
    "        'train_acc_halal': [],\n",
    "        'train_acc_healthy': [],\n",
    "        'train_acc_overall': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc_prdtype': [],\n",
    "        'val_acc_weight': [],\n",
    "        'val_acc_halal': [],\n",
    "        'val_acc_healthy': [],\n",
    "        'val_acc_overall': [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects_prdtype = 0\n",
    "            running_corrects_weight = 0\n",
    "            running_corrects_halal = 0\n",
    "            running_corrects_healthy = 0\n",
    "            running_corrects_overall = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            for inputs, label_prdtype, label_weight, label_halal, label_healthy, _ in train_loader if phase == 'train' else test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                label_prdtype = label_prdtype.to(device)\n",
    "                label_weight = label_weight.to(device)\n",
    "                label_halal = label_halal.to(device)\n",
    "                label_healthy = label_healthy.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs_prdtype, outputs_weight, outputs_halal, outputs_healthy = model(inputs)\n",
    "                    loss_prdtype = criterion_prdtype(outputs_prdtype, label_prdtype)\n",
    "                    loss_weight = criterion_weight(outputs_weight, label_weight)\n",
    "                    loss_halal = criterion_halal(outputs_halal, label_halal)\n",
    "                    loss_healthy = criterion_healthy(outputs_healthy, label_healthy)\n",
    "                    loss = loss_prdtype + loss_weight + loss_halal + loss_healthy  # Total loss\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects_prdtype += calculate_accuracy(outputs_prdtype, label_prdtype) * inputs.size(0)\n",
    "                running_corrects_weight += calculate_accuracy(outputs_weight, label_weight) * inputs.size(0)\n",
    "                running_corrects_halal += calculate_accuracy(outputs_halal, label_halal) * inputs.size(0)\n",
    "                running_corrects_healthy += calculate_accuracy(outputs_healthy, label_healthy) * inputs.size(0)\n",
    "                correct_preds_overall = ((outputs_prdtype.argmax(1) == label_prdtype) &\n",
    "                                         (outputs_weight.argmax(1) == label_weight) &\n",
    "                                         (outputs_halal.argmax(1) == label_halal) &\n",
    "                                         (outputs_healthy.argmax(1) == label_healthy))\n",
    "                running_corrects_overall += correct_preds_overall.sum().item()\n",
    "                total_samples += inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / total_samples\n",
    "            epoch_acc_prdtype = running_corrects_prdtype / total_samples\n",
    "            epoch_acc_weight = running_corrects_weight / total_samples\n",
    "            epoch_acc_halal = running_corrects_halal / total_samples\n",
    "            epoch_acc_healthy = running_corrects_healthy / total_samples\n",
    "            epoch_acc_overall = running_corrects_overall / total_samples\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc_overall))\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    print(\"new loss obtained\")\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    print(f\"tmp epochs_no_improve: {epochs_no_improve}\")\n",
    "\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered at epoch: {}\".format(epoch + 1))\n",
    "                    model.load_state_dict(best_model_wts)\n",
    "                    return model, history\n",
    "            \n",
    "            print(f\"epochs_no_improve: {epochs_no_improve}\")\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "# Example usage of the function\n",
    "# Assuming CONFIGS, trainLoader, testLoader, etc. are already defined\n",
    "num_classes_prdtype = len(np.unique(trainLabels['labels_prdtype']))\n",
    "num_classes_weight = len(np.unique(trainLabels['labels_weight']))\n",
    "num_classes_halal = len(np.unique(trainLabels['labels_halal']))\n",
    "num_classes_healthy = len(np.unique(trainLabels['labels_healthy']))\n",
    "\n",
    "custom_resnet_model = MultiHeadResNet(num_classes_prdtype, num_classes_weight, num_classes_halal, num_classes_healthy)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_resnet_model = custom_resnet_model.to(device)\n",
    "\n",
    "criterion_prdtype = nn.CrossEntropyLoss()\n",
    "criterion_weight = nn.CrossEntropyLoss()\n",
    "criterion_halal = nn.CrossEntropyLoss()\n",
    "criterion_healthy = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(custom_resnet_model.parameters(), lr=CONFIGS['INIT_LR'])\n",
    "\n",
    "criteria = (criterion_prdtype, criterion_weight, criterion_halal, criterion_healthy)\n",
    "\n",
    "# Start time\n",
    "print(\"Model training started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model_ft, history = train_model(custom_resnet_model, criteria, optimizer, trainLoader, testLoader, device, num_epochs=CONFIGS['NUM_EPOCHS'])\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "print(\"Model training completed...\")\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time spent: {round(execution_time/60,2)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fac334ef-1ac8-4172-b75a-59a13b25f7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent: 12.63 mins\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time spent: {round(execution_time/60,2)} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7c51679-7fe1-4459-b105-9692bf06a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving label encoder...\n"
     ]
    }
   ],
   "source": [
    "torch.save(model_ft.state_dict(), 'output/multi_head_model.pth')\n",
    "\n",
    "print(\"[INFO] saving label encoder...\")\n",
    "f = open(CONFIGS[\"LE_PATH_PRDTYPE\"], \"wb\")\n",
    "f.write(pickle.dumps(le_prdtype))\n",
    "f.close()\n",
    "f = open(CONFIGS[\"LE_PATH_WEIGHT\"], \"wb\")\n",
    "f.write(pickle.dumps(le_weight))\n",
    "f.close()\n",
    "f = open(CONFIGS[\"LE_PATH_HALAL\"], \"wb\")\n",
    "f.write(pickle.dumps(le_halal))\n",
    "f.close()\n",
    "f = open(CONFIGS[\"LE_PATH_HEALTHY\"], \"wb\")\n",
    "f.write(pickle.dumps(le_healthy))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de520c9-41e2-4c11-bed9-67caf2609523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracies: {'ProductType': 1.0, 'Weight': 0.9914893617021276, 'HalalStatus': 1.0, 'HealthStatus': 1.0, 'Total': 0.9914893617021276}\n",
      "Test Accuracies: {'ProductType': 0.7277227722772277, 'Weight': 0.6237623762376238, 'HalalStatus': 0.7871287128712872, 'HealthStatus': 0.9158415841584159, 'Total': 0.5148514851485149}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, dataset_size, num_mc_samples=50):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_counts = {'ProductType': 0, 'Weight': 0, 'HalalStatus': 0, 'HealthStatus': 0, 'Total': 0}\n",
    "    total_distances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, labels_prdtype, labels_weight, labels_halal, labels_healthy, filenames) in data_loader:\n",
    "            images = images.to(CONFIGS['DEVICE'])\n",
    "            labels_prdtype, labels_weight, labels_halal, labels_healthy = labels_prdtype.to(CONFIGS['DEVICE']), labels_weight.to(CONFIGS['DEVICE']), labels_halal.to(CONFIGS['DEVICE']), labels_healthy.to(CONFIGS['DEVICE'])\n",
    "\n",
    "            # Forward pass\n",
    "            out1, out2, out3, out4 = model(images)\n",
    "\n",
    "            # Store deterministic predictions\n",
    "            det_pred_prdtype = out1.argmax(1)\n",
    "            det_pred_weight = out2.argmax(1)\n",
    "            det_pred_halal = out3.argmax(1)\n",
    "            det_pred_healthy = out4.argmax(1)\n",
    "\n",
    "            # # Monte Carlo Dropout\n",
    "            # mc_distances = []\n",
    "            # model.train()  # Enable dropout\n",
    "            # for i in range(num_mc_samples):\n",
    "            #     mc_out1, mc_out2, mc_out3, mc_out4, _ = model(images)\n",
    "            #     mc_pred_prdtype = mc_out1.argmax(1)\n",
    "            #     mc_pred_weight = mc_out2.argmax(1)\n",
    "            #     mc_pred_halal = mc_out3.argmax(1)\n",
    "            #     mc_pred_healthy = mc_out4.argmax(1)\n",
    "\n",
    "            #     distance = (det_pred_prdtype != mc_pred_prdtype).float() + \\\n",
    "            #                (det_pred_weight != mc_pred_weight).float() + \\\n",
    "            #                (det_pred_halal != mc_pred_halal).float() + \\\n",
    "            #                (det_pred_healthy != mc_pred_healthy).float()\n",
    "\n",
    "            #     mc_distances.append(distance)\n",
    "\n",
    "            # # Calculate average distance\n",
    "            # avg_distance = torch.stack(mc_distances).mean(0)\n",
    "            # total_distances.extend(avg_distance.cpu().numpy().tolist())\n",
    "\n",
    "            # Restore to evaluation mode\n",
    "            # model.eval()\n",
    "\n",
    "            # Update correct counts for each category\n",
    "            correct_counts['ProductType'] += (out1.argmax(1) == labels_prdtype).float().sum().item()\n",
    "            correct_counts['Weight'] += (out2.argmax(1) == labels_weight).float().sum().item()\n",
    "            correct_counts['HalalStatus'] += (out3.argmax(1) == labels_halal).float().sum().item()\n",
    "            correct_counts['HealthStatus'] += (out4.argmax(1) == labels_healthy).float().sum().item()\n",
    "            correct_counts['Total'] += ((out1.argmax(1) == labels_prdtype) & (out2.argmax(1) == labels_weight) & (out3.argmax(1) == labels_halal) & (out4.argmax(1) == labels_healthy)).float().sum().item()\n",
    "\n",
    "    # Calculate accuracies\n",
    "    accuracies = {key: correct_counts[key] / dataset_size for key in correct_counts}\n",
    "    # avg_total_distance = sum(total_distances) / len(total_distances)\n",
    "\n",
    "    # Plot histogram of distances\n",
    "    # plt.hist(total_distances, bins=30, alpha=0.7, label='Total Distances', color='b')\n",
    "    # plt.xlabel('Distance')\n",
    "    # plt.ylabel('Frequency')\n",
    "    # plt.title(f'Distribution of Total Distances: N={len(total_distances)}')\n",
    "    # plt.show()\n",
    "\n",
    "    # return accuracies, avg_total_distance, total_distances\n",
    "    return accuracies\n",
    "\n",
    "# Evaluate on training set\n",
    "train_accuracies = evaluate_model(model_ft, trainLoader, len(trainDS))\n",
    "print(f\"Training Accuracies: {train_accuracies}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracies= evaluate_model(model_ft, testLoader, len(testDS))\n",
    "print(f\"Test Accuracies: {test_accuracies}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1911e938-4e3e-4d1c-b7a7-79ea6b2d7324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>CorrectTotalLabel</th>\n",
       "      <th>ProductType_AdultMilk</th>\n",
       "      <th>ProductType_BabyMilkPowder</th>\n",
       "      <th>ProductType_Babyfood</th>\n",
       "      <th>ProductType_BeehoonVermicelliMeesua</th>\n",
       "      <th>ProductType_BiscuitsCrackersCookies</th>\n",
       "      <th>ProductType_Book</th>\n",
       "      <th>ProductType_BreakfastCerealsCornflakes</th>\n",
       "      <th>ProductType_CannedPacketCreamersSweet</th>\n",
       "      <th>...</th>\n",
       "      <th>Weight_400-499g</th>\n",
       "      <th>Weight_500-599g</th>\n",
       "      <th>Weight_600-699g</th>\n",
       "      <th>Weight_700-799g</th>\n",
       "      <th>Weight_800-899g</th>\n",
       "      <th>Weight_900-999g</th>\n",
       "      <th>HalalStatus_Halal</th>\n",
       "      <th>HalalStatus_NonHalal</th>\n",
       "      <th>HealthStatus_Healthy</th>\n",
       "      <th>HealthStatus_NonHealthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_0722_jpeg.rf.d225be9cf3e9a21b88a9111c79c48...</td>\n",
       "      <td>OtherNoodles_400-499g_NonHalal_NonHealthy</td>\n",
       "      <td>-0.042604</td>\n",
       "      <td>-0.917904</td>\n",
       "      <td>-1.518212</td>\n",
       "      <td>-0.296711</td>\n",
       "      <td>2.354687</td>\n",
       "      <td>-2.620731</td>\n",
       "      <td>-2.827710</td>\n",
       "      <td>-3.824989</td>\n",
       "      <td>...</td>\n",
       "      <td>7.529936</td>\n",
       "      <td>-0.084116</td>\n",
       "      <td>-1.743978</td>\n",
       "      <td>-5.213832</td>\n",
       "      <td>-1.647417</td>\n",
       "      <td>-1.764002</td>\n",
       "      <td>-3.006634</td>\n",
       "      <td>2.808640</td>\n",
       "      <td>-5.266931</td>\n",
       "      <td>5.735337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_2125_jpeg.rf.98ceaf3474e8a755edb2d0c969c93...</td>\n",
       "      <td>BiscuitsCrackersCookies_500-599g_NonHalal_NonH...</td>\n",
       "      <td>-3.947608</td>\n",
       "      <td>0.006503</td>\n",
       "      <td>0.566163</td>\n",
       "      <td>-1.790939</td>\n",
       "      <td>9.116190</td>\n",
       "      <td>-4.677173</td>\n",
       "      <td>-5.523167</td>\n",
       "      <td>-5.752591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353897</td>\n",
       "      <td>10.116268</td>\n",
       "      <td>-3.711792</td>\n",
       "      <td>-4.142940</td>\n",
       "      <td>-1.033993</td>\n",
       "      <td>-4.143282</td>\n",
       "      <td>-3.573488</td>\n",
       "      <td>3.979955</td>\n",
       "      <td>-3.407155</td>\n",
       "      <td>3.641886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20231222_0419.jpg</td>\n",
       "      <td>SweetsChocolatesOthers_200-299g_Halal_NonHealthy</td>\n",
       "      <td>-3.898118</td>\n",
       "      <td>-1.853065</td>\n",
       "      <td>-1.576361</td>\n",
       "      <td>-1.745042</td>\n",
       "      <td>0.757353</td>\n",
       "      <td>-1.532205</td>\n",
       "      <td>-6.475076</td>\n",
       "      <td>-2.823888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103763</td>\n",
       "      <td>-1.004154</td>\n",
       "      <td>-3.132432</td>\n",
       "      <td>-2.784842</td>\n",
       "      <td>-1.196354</td>\n",
       "      <td>-4.022461</td>\n",
       "      <td>3.352356</td>\n",
       "      <td>-2.715928</td>\n",
       "      <td>-4.029167</td>\n",
       "      <td>5.381159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023_10_25_11_49_41_382262.jpg</td>\n",
       "      <td>BabyMilkPowder_400-499g_Halal_NonHealthy</td>\n",
       "      <td>0.379034</td>\n",
       "      <td>9.634521</td>\n",
       "      <td>0.752696</td>\n",
       "      <td>-3.017066</td>\n",
       "      <td>-0.769694</td>\n",
       "      <td>-3.412030</td>\n",
       "      <td>-2.079742</td>\n",
       "      <td>-1.423823</td>\n",
       "      <td>...</td>\n",
       "      <td>6.357276</td>\n",
       "      <td>-1.178355</td>\n",
       "      <td>-3.272727</td>\n",
       "      <td>-3.531720</td>\n",
       "      <td>3.178313</td>\n",
       "      <td>-0.548229</td>\n",
       "      <td>3.688625</td>\n",
       "      <td>-2.877854</td>\n",
       "      <td>-2.247989</td>\n",
       "      <td>2.201218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023_8_11_12_16_13_156049_png.rf.d1b4db49f97ab...</td>\n",
       "      <td>FlavoredMilk_1-99g_Halal_Healthy</td>\n",
       "      <td>-1.101073</td>\n",
       "      <td>-0.451816</td>\n",
       "      <td>-1.461948</td>\n",
       "      <td>0.367423</td>\n",
       "      <td>0.476725</td>\n",
       "      <td>-4.617911</td>\n",
       "      <td>-2.182668</td>\n",
       "      <td>-2.599943</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.091295</td>\n",
       "      <td>-3.417022</td>\n",
       "      <td>-2.225312</td>\n",
       "      <td>-1.328926</td>\n",
       "      <td>-0.339995</td>\n",
       "      <td>-0.344110</td>\n",
       "      <td>4.364552</td>\n",
       "      <td>-3.306172</td>\n",
       "      <td>3.103263</td>\n",
       "      <td>-0.926259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Filename  \\\n",
       "0  IMG_0722_jpeg.rf.d225be9cf3e9a21b88a9111c79c48...   \n",
       "1  IMG_2125_jpeg.rf.98ceaf3474e8a755edb2d0c969c93...   \n",
       "2                                  20231222_0419.jpg   \n",
       "3                     2023_10_25_11_49_41_382262.jpg   \n",
       "4  2023_8_11_12_16_13_156049_png.rf.d1b4db49f97ab...   \n",
       "\n",
       "                                   CorrectTotalLabel  ProductType_AdultMilk  \\\n",
       "0          OtherNoodles_400-499g_NonHalal_NonHealthy              -0.042604   \n",
       "1  BiscuitsCrackersCookies_500-599g_NonHalal_NonH...              -3.947608   \n",
       "2   SweetsChocolatesOthers_200-299g_Halal_NonHealthy              -3.898118   \n",
       "3           BabyMilkPowder_400-499g_Halal_NonHealthy               0.379034   \n",
       "4                   FlavoredMilk_1-99g_Halal_Healthy              -1.101073   \n",
       "\n",
       "   ProductType_BabyMilkPowder  ProductType_Babyfood  \\\n",
       "0                   -0.917904             -1.518212   \n",
       "1                    0.006503              0.566163   \n",
       "2                   -1.853065             -1.576361   \n",
       "3                    9.634521              0.752696   \n",
       "4                   -0.451816             -1.461948   \n",
       "\n",
       "   ProductType_BeehoonVermicelliMeesua  ProductType_BiscuitsCrackersCookies  \\\n",
       "0                            -0.296711                             2.354687   \n",
       "1                            -1.790939                             9.116190   \n",
       "2                            -1.745042                             0.757353   \n",
       "3                            -3.017066                            -0.769694   \n",
       "4                             0.367423                             0.476725   \n",
       "\n",
       "   ProductType_Book  ProductType_BreakfastCerealsCornflakes  \\\n",
       "0         -2.620731                               -2.827710   \n",
       "1         -4.677173                               -5.523167   \n",
       "2         -1.532205                               -6.475076   \n",
       "3         -3.412030                               -2.079742   \n",
       "4         -4.617911                               -2.182668   \n",
       "\n",
       "   ProductType_CannedPacketCreamersSweet  ...  Weight_400-499g  \\\n",
       "0                              -3.824989  ...         7.529936   \n",
       "1                              -5.752591  ...         0.353897   \n",
       "2                              -2.823888  ...        -0.103763   \n",
       "3                              -1.423823  ...         6.357276   \n",
       "4                              -2.599943  ...        -1.091295   \n",
       "\n",
       "   Weight_500-599g  Weight_600-699g  Weight_700-799g  Weight_800-899g  \\\n",
       "0        -0.084116        -1.743978        -5.213832        -1.647417   \n",
       "1        10.116268        -3.711792        -4.142940        -1.033993   \n",
       "2        -1.004154        -3.132432        -2.784842        -1.196354   \n",
       "3        -1.178355        -3.272727        -3.531720         3.178313   \n",
       "4        -3.417022        -2.225312        -1.328926        -0.339995   \n",
       "\n",
       "   Weight_900-999g  HalalStatus_Halal  HalalStatus_NonHalal  \\\n",
       "0        -1.764002          -3.006634              2.808640   \n",
       "1        -4.143282          -3.573488              3.979955   \n",
       "2        -4.022461           3.352356             -2.715928   \n",
       "3        -0.548229           3.688625             -2.877854   \n",
       "4        -0.344110           4.364552             -3.306172   \n",
       "\n",
       "   HealthStatus_Healthy  HealthStatus_NonHealthy  \n",
       "0             -5.266931                 5.735337  \n",
       "1             -3.407155                 3.641886  \n",
       "2             -4.029167                 5.381159  \n",
       "3             -2.247989                 2.201218  \n",
       "4              3.103263                -0.926259  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model2(model, data_loader, le_prdtype, le_weight, le_halal, le_healthy):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images, labels_prdtype, labels_weight, labels_halal, labels_healthy, filenames) in data_loader:\n",
    "            images = images.to(CONFIGS['DEVICE'])\n",
    "            out1, out2, out3, out4 = model(images)\n",
    "\n",
    "            for idx in range(len(filenames)):\n",
    "                correct_label = f\"{le_prdtype.classes_[labels_prdtype[idx]]}_{le_weight.classes_[labels_weight[idx]]}_{le_halal.classes_[labels_halal[idx]]}_{le_healthy.classes_[labels_healthy[idx]]}\"\n",
    "                row = [filenames[idx], correct_label]\n",
    "                row.extend(out1[idx].cpu().numpy())\n",
    "                row.extend(out2[idx].cpu().numpy())\n",
    "                row.extend(out3[idx].cpu().numpy())\n",
    "                row.extend(out4[idx].cpu().numpy())\n",
    "                results.append(row)\n",
    "\n",
    "    # Define column names\n",
    "    column_names = ['Filename', 'CorrectTotalLabel']\n",
    "    column_names += ['ProductType_' + name for name in le_prdtype.classes_]\n",
    "    column_names += ['Weight_' + name for name in le_weight.classes_]\n",
    "    column_names += ['HalalStatus_' + name for name in le_halal.classes_]\n",
    "    column_names += ['HealthStatus_' + name for name in le_healthy.classes_]\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=column_names)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Usage of the function\n",
    "train_results_df = evaluate_model2(model_ft, trainLoader, le_prdtype, le_weight, le_halal, le_healthy)\n",
    "test_results_df = evaluate_model2(model_ft, testLoader, le_prdtype, le_weight, le_halal, le_healthy)\n",
    "\n",
    "# Concatenate the training and test results\n",
    "combined_results_df = pd.concat([train_results_df, test_results_df], axis=0)\n",
    "combined_results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the combined results\n",
    "print(\"Combined Results:\")\n",
    "combined_results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9c7f644-2873-424e-a5d0-a5449bc59145",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results_df.to_csv('new_imgs_results_small_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3dfc17-58da-4629-ac6d-5b3b79cf68b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc09a3-dc99-430b-9191-ad921bae3108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f73dcb-5826-4883-8db9-4597a6029a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292ad04-327e-4c81-9f06-4cde6bcc4d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f67b0-153b-43ae-98c0-3089dfb4695a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15a2d4-c1a8-45f9-b9c1-3f7654d15285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac763cb0-09bf-49dc-bad0-ca99b577e356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[': 1750,\n",
       "         '3': 771,\n",
       "         ',': 3500,\n",
       "         ' ': 3500,\n",
       "         '7': 736,\n",
       "         ']': 1750,\n",
       "         '6': 1142,\n",
       "         '0': 650,\n",
       "         '1': 740,\n",
       "         '2': 840,\n",
       "         '5': 360,\n",
       "         '4': 11})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Since the actual data is an image of text, we'll manually transcribe a few lines to demonstrate the process.\n",
    "# In practice, the user would extract the text data using OCR (Optical Character Recognition) tools such as Tesseract.\n",
    "\n",
    "\n",
    "# Convert the example data into a DataFrame\n",
    "df = pd.read_csv(\"./results_[0.1].csv\")\n",
    "\n",
    "# Flatten the list of lists into a single list\n",
    "all_indices = [i for sublist in df['Alpha Max Indices'] for i in sublist]\n",
    "\n",
    "# Count the frequency of each number\n",
    "frequency_counts = Counter(all_indices)\n",
    "\n",
    "frequency_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff025fa-23be-4979-a298-6bd8ecc42a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
