{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa8221f-21c5-4d11-9a14-3430af0b2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.models import detection, resnet50, ResNet50_Weights\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc75a738-e9c9-4b95-9229-0540fe0a2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGS = {\n",
    "    # determine the current device and based on that set the pin memory\n",
    "    # flag\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # specify ImageNet mean and standard deviation\n",
    "    \"IMG_MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"IMG_STD\": [0.229, 0.224, 0.225],\n",
    "    \"MC_DROPOUT_ENABLED\": False,  # Switch to enable/disable MC Dropout for confidence score\n",
    "    \"NUM_DROPOUT_RUNS\": 3,\n",
    "    \"CONFIDENCE_THRESHOLD\": 0,\n",
    "    \"BIG_MODEL_IMG_SIZE\": 320,\n",
    "    \"SMALL_MODEL_IMG_SIZE\": 60,\n",
    "    \"MEAN_PRIOR\": -15,\n",
    "    \"MODEL_PATH\": 'traindatawithin1'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e0656-aacb-422f-af50-38126e838d71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Big model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce9ac3-24b6-4306-ae9f-b4d73e4162b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f349c065-57dd-47f2-9043-a91308360219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liupeng/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadResNet_BigModel(nn.Module):\n",
    "    def __init__(self, num_classes_prdtype, num_classes_weight, num_classes_halal, num_classes_healthy):\n",
    "        super(MultiHeadResNet_BigModel, self).__init__()\n",
    "        self.base_model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        num_ftrs = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "        # Define custom fully connected layers for each prediction head\n",
    "        self.fc_prdtype = nn.Linear(num_ftrs, num_classes_prdtype)\n",
    "        self.fc_weight = nn.Linear(num_ftrs, num_classes_weight)\n",
    "        self.fc_halal = nn.Linear(num_ftrs, num_classes_halal)\n",
    "        self.fc_healthy = nn.Linear(num_ftrs, num_classes_healthy)\n",
    "        self.fc_bbox = nn.Linear(num_ftrs, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        prdtype = self.fc_prdtype(x)\n",
    "        weight = self.fc_weight(x)\n",
    "        halal = self.fc_halal(x)\n",
    "        healthy = self.fc_healthy(x)\n",
    "        box = self.fc_bbox(x)\n",
    "        return prdtype, weight, halal, healthy, box\n",
    "\n",
    "    \n",
    "# load label encoder \n",
    "def load_label_encoder_big_model():\n",
    "    le_prdtype = pickle.loads(open(os.path.join('../../NN_model/model_weights', CONFIGS['MODEL_PATH'], 'le_prdtype.pickle'), \"rb\").read())\n",
    "    le_weight = pickle.loads(open(os.path.join('../../NN_model/model_weights', CONFIGS['MODEL_PATH'], 'le_weight.pickle'), \"rb\").read())\n",
    "    le_halal = pickle.loads(open(os.path.join('../../NN_model/model_weights', CONFIGS['MODEL_PATH'], 'le_halal.pickle'), \"rb\").read())\n",
    "    le_healthy = pickle.loads(open(os.path.join('../../NN_model/model_weights', CONFIGS['MODEL_PATH'], 'le_healthy.pickle'), \"rb\").read())\n",
    "    \n",
    "    return le_prdtype, le_weight, le_halal, le_healthy\n",
    "\n",
    "le_prdtype, le_weight, le_halal, le_healthy = load_label_encoder_big_model()\n",
    "\n",
    "# Load the trained MultiHeadResNet model\n",
    "def load_model():\n",
    "    # Verify the number of classes for each label\n",
    "    num_classes_prdtype = len(le_prdtype.classes_)\n",
    "    num_classes_weight = len(le_weight.classes_)\n",
    "    num_classes_halal = len(le_halal.classes_)\n",
    "    num_classes_healthy = len(le_healthy.classes_)\n",
    "    # print(num_classes_prdtype)\n",
    "    # print(num_classes_healthy)\n",
    "\n",
    "    custom_resnet_model = MultiHeadResNet_BigModel(\n",
    "        num_classes_prdtype=num_classes_prdtype,\n",
    "        num_classes_weight=num_classes_weight,\n",
    "        num_classes_halal=num_classes_halal,\n",
    "        num_classes_healthy=num_classes_healthy\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join('../../NN_model/model_weights', CONFIGS['MODEL_PATH'], 'multi_head_model.pth')\n",
    "    # print(\"test1\")\n",
    "    if os.path.exists(model_path):\n",
    "        custom_resnet_model.load_state_dict(torch.load(model_path, map_location=CONFIGS['DEVICE']))\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    # print(\"test2\")\n",
    "    custom_resnet_model.to(CONFIGS['DEVICE'])\n",
    "    custom_resnet_model.eval()\n",
    "    return custom_resnet_model\n",
    "\n",
    "big_model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa0d993-323b-42cd-b742-bc571f747246",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scoring on main imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29399c3-77e0-488c-afee-39967ddb641a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1433660164.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    main_imgs_results_big_model.head()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "main_imgs_results_big_model = pd.read_csv(os.path.join('../../NN_model/model_weights', CONFIGS['MODEL_PATH'], 'main_imgs_results_big_model.csv'))\n",
    "main_imgs_results_big_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98391a-f54d-4251-a8c3-80daa4560e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the current column names to a list\n",
    "new_columns = main_imgs_results_big_model.columns.tolist()\n",
    "\n",
    "# Modify the first two elements\n",
    "new_columns[0] = 'filepath'\n",
    "new_columns[1] = 'label'\n",
    "\n",
    "# Assign the modified list of column names back to the DataFrame\n",
    "main_imgs_results_big_model.columns = new_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c1140-589c-4e62-812d-432b1e1b28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_imgs_results_big_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0881c3c5-113b-4447-9c10-d6988d6f361d",
   "metadata": {},
   "source": [
    "# Scoring on unused training imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b0320-4570-4ea6-85bf-f863a8f4d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(\"../../master_list.csv\")\n",
    "master_df = master_df[master_df['remove']!=1]\n",
    "master_df.reset_index(drop=True, inplace=True)\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9201cee5-72b3-4bf5-8ac5-894485b5858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(main_imgs_results_big_model[\"filepath\"].tolist())\n",
    "b = set(master_df[\"filepath\"].tolist())\n",
    "pending_imgs = [i for i in b if i not in a]\n",
    "len(pending_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c96d3-7f57-43b3-b197-065c5a0146db",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CONFIGS['IMG_MEAN'], std=CONFIGS['IMG_STD'])\n",
    "])\n",
    "\n",
    "new_imgs_results_big_model = []  # List to store the results\n",
    "\n",
    "for row in pending_imgs:\n",
    "    image_path = \"../../rshiny/www/all_images/\" + row\n",
    "    frame = cv2.imread(image_path)\n",
    "\n",
    "    # Preprocessing steps\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (CONFIGS['BIG_MODEL_IMG_SIZE'], CONFIGS['BIG_MODEL_IMG_SIZE']))\n",
    "    frame = frame.transpose((2, 0, 1))\n",
    "    frame = torch.from_numpy(frame).float()\n",
    "    frame = transforms_test(frame).unsqueeze(0).to(CONFIGS['DEVICE'])\n",
    "\n",
    "    # Perform prediction\n",
    "    with torch.no_grad():\n",
    "        out1, out2, out3, out4, _ = big_model(frame)\n",
    "\n",
    "    # reference the correct label from master list\n",
    "    tmp_correct_label = master_df.loc[master_df['filepath'] == row, 'label'].iloc[0]\n",
    "        \n",
    "    # Extract and store the results\n",
    "    prediction_row = [row, tmp_correct_label]\n",
    "    prediction_row.extend(out1.cpu().numpy().flatten())\n",
    "    prediction_row.extend(out2.cpu().numpy().flatten())\n",
    "    prediction_row.extend(out3.cpu().numpy().flatten())\n",
    "    prediction_row.extend(out4.cpu().numpy().flatten())\n",
    "    new_imgs_results_big_model.append(prediction_row)\n",
    "\n",
    "\n",
    "# Define column names for the new DataFrame\n",
    "column_names = ['filepath', 'label']\n",
    "big_model_pred_col_name_original = main_imgs_results_big_model.columns[2:].tolist()\n",
    "column_names += big_model_pred_col_name_original\n",
    "\n",
    "# Create the DataFrame\n",
    "new_imgs_results_big_model = pd.DataFrame(new_imgs_results_big_model, columns=column_names)\n",
    "new_imgs_results_big_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79855889-bc7b-4ac6-83b0-21ff404a1220",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scoring on new imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638f976-5416-43be-9e88-db876766869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_imgs_df = pd.read_csv(\"GPT_model/chatgpt_prediction.csv\")\n",
    "# new_imgs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff0834-2729-458c-a450-ebab96889095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms_test = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=CONFIGS['IMG_MEAN'], std=CONFIGS['IMG_STD'])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01814b1-4a72-44aa-9c3e-6bd08ff5a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_df = pd.read_csv(\"../../master_list.csv\")\n",
    "# master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf6320c-d0c1-4ed6-940c-9b0b74b5b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_imgs_results_big_model = []  # List to store the results\n",
    "\n",
    "# for idx, row in new_imgs_df.iterrows():\n",
    "#     image_path = \"../../all_images/\" + row['img_filename']\n",
    "#     frame = cv2.imread(image_path)\n",
    "\n",
    "#     # Preprocessing steps\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     frame = cv2.resize(frame, (CONFIGS['BIG_MODEL_IMG_SIZE'], CONFIGS['BIG_MODEL_IMG_SIZE']))\n",
    "#     frame = frame.transpose((2, 0, 1))\n",
    "#     frame = torch.from_numpy(frame).float()\n",
    "#     frame = transforms_test(frame).unsqueeze(0).to(CONFIGS['DEVICE'])\n",
    "\n",
    "#     # Perform prediction\n",
    "#     with torch.no_grad():\n",
    "#         out1, out2, out3, out4, _ = big_model(frame)\n",
    "\n",
    "#     # reference the correct label from master list\n",
    "#     tmp_correct_label = master_df.loc[master_df['filepath'] == row['img_filename'], 'label'].iloc[0]\n",
    "        \n",
    "#     # Extract and store the results\n",
    "#     prediction_row = [row['img_filename'], tmp_correct_label]\n",
    "#     prediction_row.extend(out1.cpu().numpy().flatten())\n",
    "#     prediction_row.extend(out2.cpu().numpy().flatten())\n",
    "#     prediction_row.extend(out3.cpu().numpy().flatten())\n",
    "#     prediction_row.extend(out4.cpu().numpy().flatten())\n",
    "#     new_imgs_results_big_model.append(prediction_row)\n",
    "\n",
    "\n",
    "# # Define column names for the new DataFrame\n",
    "# column_names = ['filepath', 'label']\n",
    "# big_model_pred_col_name_original = main_imgs_results_big_model.columns[2:].tolist()\n",
    "# column_names += big_model_pred_col_name_original\n",
    "\n",
    "# # Create the DataFrame\n",
    "# new_imgs_results_big_model = pd.DataFrame(new_imgs_results_big_model, columns=column_names)\n",
    "# new_imgs_results_big_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64815a97-84b8-4fc9-b2e1-1e0488b944f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_imgs_results_big_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe5621-33fa-440c-8451-43ce13bcd1eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## All scorings from big model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6720b1d-00d1-4f93-8b73-c52711f06664",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_imgs_results_big_model['img_type'] = \"existing\"\n",
    "new_imgs_results_big_model['img_type'] = \"new\"\n",
    "all_imgs_results_big_model = pd.concat([main_imgs_results_big_model, new_imgs_results_big_model], axis=0)\n",
    "all_imgs_results_big_model.reset_index(drop=True, inplace=True)\n",
    "all_imgs_results_big_model.head()\n",
    "# all_imgs_results_big_model = main_imgs_results_big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d68c5e-ef8e-426d-880d-c57bafa97219",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_imgs_results_big_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665e4ab-b3c7-40f3-b37d-c6c666337610",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_imgs_results_big_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca4212-1273-47d5-a1c8-6ba880ad6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_results_big_model.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0a518-f283-4373-bdc9-0137b9bacf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_results_big_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7cbc6-94c5-4f86-ae03-576c50ed8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_results_big_model.to_csv(os.path.join('../../NN_model/model_weights', CONFIGS['MODEL_PATH'], 'all_imgs_results_big_model.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c46de7-9070-4629-8358-2abd00bffa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2e05e-70db-4b6f-835a-8dcd3f7bbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadResNet_SmallModel(nn.Module):\n",
    "#     def __init__(self, num_classes_prdtype, num_classes_weight, num_classes_halal, num_classes_healthy):\n",
    "#         super(MultiHeadResNet_SmallModel, self).__init__()\n",
    "#         self.base_model = models.resnet18(pretrained=True)\n",
    "#         num_ftrs = self.base_model.fc.in_features\n",
    "#         self.base_model.fc = nn.Identity()\n",
    "\n",
    "#         # Define custom fully connected layers for each prediction head\n",
    "#         self.fc_prdtype = nn.Linear(num_ftrs, num_classes_prdtype)\n",
    "#         self.fc_weight = nn.Linear(num_ftrs, num_classes_weight)\n",
    "#         self.fc_halal = nn.Linear(num_ftrs, num_classes_halal)\n",
    "#         self.fc_healthy = nn.Linear(num_ftrs, num_classes_healthy)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.base_model(x)\n",
    "#         prdtype = self.fc_prdtype(x)\n",
    "#         weight = self.fc_weight(x)\n",
    "#         halal = self.fc_halal(x)\n",
    "#         healthy = self.fc_healthy(x)\n",
    "#         return prdtype, weight, halal, healthy\n",
    "\n",
    "    \n",
    "# # load label encoder \n",
    "# def load_label_encoder_small_model():\n",
    "#     le_prdtype = pickle.loads(open(\"../small_model/output/le_prdtype.pickle\", \"rb\").read())\n",
    "#     le_weight = pickle.loads(open(\"../small_model/output/le_weight.pickle\", \"rb\").read())\n",
    "#     le_halal = pickle.loads(open(\"../small_model/output/le_halal.pickle\", \"rb\").read())\n",
    "#     le_healthy = pickle.loads(open(\"../small_model/output/le_healthy.pickle\", \"rb\").read())\n",
    "    \n",
    "#     return le_prdtype, le_weight, le_halal, le_healthy\n",
    "\n",
    "# le_prdtype, le_weight, le_halal, le_healthy = load_label_encoder_small_model()\n",
    "\n",
    "# # Load the trained MultiHeadResNet model\n",
    "# def load_model():\n",
    "#     # Verify the number of classes for each label\n",
    "#     num_classes_prdtype = len(le_prdtype.classes_)\n",
    "#     num_classes_weight = len(le_weight.classes_)\n",
    "#     num_classes_halal = len(le_halal.classes_)\n",
    "#     num_classes_healthy = len(le_healthy.classes_)\n",
    "#     # print(num_classes_prdtype)\n",
    "#     # print(num_classes_healthy)\n",
    "\n",
    "#     custom_resnet_model = MultiHeadResNet_SmallModel(\n",
    "#         num_classes_prdtype=num_classes_prdtype,\n",
    "#         num_classes_weight=num_classes_weight,\n",
    "#         num_classes_halal=num_classes_halal,\n",
    "#         num_classes_healthy=num_classes_healthy\n",
    "#     )\n",
    "\n",
    "#     model_path = '../small_model/output/multi_head_model.pth'\n",
    "#     # print(\"test1\")\n",
    "#     if os.path.exists(model_path):\n",
    "#         custom_resnet_model.load_state_dict(torch.load(model_path, map_location=CONFIGS['DEVICE']))\n",
    "#     else:\n",
    "#         raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "#     # print(\"test2\")\n",
    "#     custom_resnet_model.to(CONFIGS['DEVICE'])\n",
    "#     custom_resnet_model.eval()\n",
    "#     return custom_resnet_model\n",
    " \n",
    "# small_model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01625dd-3e23-4cc8-b898-990df6e36051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_imgs_df = pd.read_csv(\"../small_model/new_imgs_list.csv\")\n",
    "# new_imgs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # ADHOC: change the new imgs to existing type\n",
    "# new_imgs_df['label'] = 'AdultMilk_1-99g_Halal_NonHealthy'\n",
    "# new_imgs_df['ProductType'] = 'AdultMilk'\n",
    "# new_imgs_df['Weight'] = '1-99g'\n",
    "# new_imgs_df['HalalStatus'] = 'Halal'\n",
    "# new_imgs_df['HealthStatus'] = 'NonHealthy'\n",
    "\n",
    "# new_imgs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10219e89-1b08-4aba-8a6b-e7371f767fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_imgs_results_small_model = pd.read_csv(\"../small_model/new_imgs_results_small_model.csv\")\n",
    "# new_imgs_results_small_model = new_imgs_results_small_model.loc[new_imgs_results_small_model.Filename.isin(new_imgs_df.filepath)]\n",
    "# new_imgs_results_small_model.reset_index(drop=True, inplace=True)\n",
    "# new_imgs_results_small_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270de8d-3462-4096-a0fc-79ad483c9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a copy of the current column names to a list\n",
    "# new_columns = new_imgs_results_small_model.columns.tolist()\n",
    "\n",
    "# # Modify the first two elements\n",
    "# new_columns[0] = 'filepath'\n",
    "# new_columns[1] = 'label'\n",
    "\n",
    "# # Assign the modified list of column names back to the DataFrame\n",
    "# new_imgs_results_small_model.columns = new_columns\n",
    "# new_imgs_results_small_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da044c-3652-47fa-b2a6-29851651a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if any name from 'extracted_names' is not in 'df' and add it as a new column\n",
    "# new_prdtype = list(set(all_imgs_results_big_model.columns) - set(new_imgs_results_small_model.columns))\n",
    "\n",
    "# if len(new_prdtype)>0:\n",
    "#     for col in new_prdtype:\n",
    "#         new_imgs_results_small_model[col] = np.random.normal(loc=CONFIGS[\"MEAN_PRIOR\"], scale=np.sqrt(0.1), size=new_imgs_results_small_model.shape[0])  # Initialize new columns\n",
    "\n",
    "# new_imgs_results_small_model.head()  # Display the updated DataFrame for verificatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835fa19-510a-46da-9e9f-fbc7f17aebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_imgs_master_list = pd.read_csv(\"../master_list.csv\")\n",
    "# main_imgs_master_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c9993-4204-4ce8-a94d-d299cc71cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_imgs_results_small_model = []  # List to store the results\n",
    "# le_prdtype, le_weight, le_halal, le_healthy = load_label_encoder_small_model()\n",
    "\n",
    "# for idx, row in main_imgs_master_list.iterrows():\n",
    "#     image_path = \"../all_images/\" + row['filepath']\n",
    "#     frame = cv2.imread(image_path)\n",
    "\n",
    "#     # Preprocessing steps\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     frame = cv2.resize(frame, (CONFIGS['SMALL_MODEL_IMG_SIZE'], CONFIGS['SMALL_MODEL_IMG_SIZE']))\n",
    "#     frame = frame.transpose((2, 0, 1))\n",
    "#     frame = torch.from_numpy(frame).float()\n",
    "#     frame = transforms_test(frame).unsqueeze(0).to(CONFIGS['DEVICE'])\n",
    "\n",
    "#     # Perform prediction\n",
    "#     with torch.no_grad():\n",
    "#         out1, out2, out3, out4 = small_model(frame)\n",
    "    \n",
    "#     # Extract and store the results\n",
    "#     prediction_row = [row['filepath'], row['label']]\n",
    "#     prediction_row.extend(out1.cpu().numpy().flatten())\n",
    "#     prediction_row.extend(out2.cpu().numpy().flatten())\n",
    "#     prediction_row.extend(out3.cpu().numpy().flatten())\n",
    "#     prediction_row.extend(out4.cpu().numpy().flatten())\n",
    "#     main_imgs_results_small_model.append(prediction_row)\n",
    "\n",
    "\n",
    "# # Define column names for the new DataFrame\n",
    "# column_names = ['filepath', 'label']\n",
    "# column_names += ['ProductType_' + name for name in le_prdtype.classes_]\n",
    "# column_names += ['Weight_' + name for name in le_weight.classes_]\n",
    "# column_names += ['HalalStatus_' + name for name in le_halal.classes_]\n",
    "# column_names += ['HealthStatus_' + name for name in le_healthy.classes_]\n",
    "\n",
    "\n",
    "# # Create the DataFrame\n",
    "# main_imgs_results_small_model = pd.DataFrame(main_imgs_results_small_model, columns=column_names)\n",
    "# main_imgs_results_small_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f296f0b-82cb-464e-b6d1-b8305eac4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if any name from 'extracted_names' is not in 'df' and add it as a new column\n",
    "# new_prdtype = list(set(all_imgs_results_big_model.columns) - set(main_imgs_results_small_model.columns))\n",
    "\n",
    "# if len(new_prdtype)>0:\n",
    "#     for col in new_prdtype:\n",
    "#         main_imgs_results_small_model[col] = np.random.normal(loc=CONFIGS[\"MEAN_PRIOR\"], scale=np.sqrt(0.1), size=main_imgs_results_small_model.shape[0])  # Initialize new columns\n",
    "\n",
    "# main_imgs_results_small_model.head()  # Display the updated DataFrame for verificatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff4b81-3c83-4f14-a4cd-0d6aca30be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_imgs_results_small_model['img_type'] = \"existing\"\n",
    "# new_imgs_results_small_model['img_type'] = \"new\"\n",
    "# all_imgs_results_small_model = pd.concat([main_imgs_results_small_model, new_imgs_results_small_model], axis=0)\n",
    "# all_imgs_results_small_model.reset_index(drop=True, inplace=True)\n",
    "# all_imgs_results_small_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb963e6-89b5-402d-a540-416aac8469e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_imgs_results_small_model.to_csv(\"all_imgs_results_small_model.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747afa2a-f3df-400e-bb42-268ea0c00573",
   "metadata": {},
   "source": [
    "# Scoring using updated Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e4275-4c63-4d61-810e-e99673be5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test images of the same product and different angles\n",
    "test_img_paths = [\"IMG_20230428_123708_jpg.rf.141ecd0cefaea75c0b7a5f281475dd6b.jpg\",\n",
    "                 \"IMG_20230428_123659_jpg.rf.5e1b6c4caabe48cf36003cb4184ff380.jpg\",\n",
    "                 \"IMG_20230428_123704_jpg.rf.5fc2415d06061ea102ef125a37bbc88c.jpg\",\n",
    "                 \"IMG_20230428_123703_jpg.rf.6a9c54175f59238cdc83999cdee6dad4.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95b3eb-9aac-4fa2-9362-7aa7e0ee7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# Create HTML for images\n",
    "html_images = ''.join([f'<img src=\"../../rshiny/www/all_images/{path}\" width=\"150\" />' for path in test_img_paths])\n",
    "\n",
    "# Display images\n",
    "display(HTML(html_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515f1cd-51a3-4ef1-a51c-59fbe4bfb6f1",
   "metadata": {},
   "source": [
    "## Prepare average logits on product type prediction from big model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4bdc3-515b-41a0-8258-ce378208d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs_results_big_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375eb524-ac5f-417d-820d-fe361d27f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model_input = all_imgs_results_big_model[all_imgs_results_big_model.filepath.isin(test_img_paths)]\n",
    "big_model_input.reset_index(inplace=True, drop=True)\n",
    "big_model_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809a25c-6538-4690-a787-017435f61d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec89ab-2b7a-42dc-bf8a-bea36ad01d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_col_name(row):\n",
    "    return row.idxmax()\n",
    "\n",
    "# individual predictions\n",
    "big_model_input.filter(like=\"ProductType\").apply(get_max_col_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772369ad-22ed-4c9e-855d-c331904f3ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions using average logits\n",
    "big_model_input.filter(like=\"ProductType\").mean().idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3491f-b211-4f44-a570-83d9c1e1a541",
   "metadata": {},
   "source": [
    "## Prepare predictions on product type and image quality from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d3fe6-8d9d-48c2-90a5-f8eab316cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the first image to call gpt\n",
    "import base64\n",
    "import requests\n",
    "import sys\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encodes an image to Base64.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "with open('/Users/liupeng/Desktop/Research/api.txt', 'r') as file:\n",
    "    api_key = file.read()\n",
    "\n",
    "image_path = os.path.join(\"../../rshiny/www/all_images/\", test_img_paths[0])\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "payload = {\n",
    "          \"model\": \"gpt-4-vision-preview\",\n",
    "          \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": \"\"\" \n",
    "                  For this image, can you make a prediction for the following four labels? \n",
    "                  The product type is based on the appearance of the product, \n",
    "                  the weight is to recognize how heavy the product is by identify the weight information on the appearance,\n",
    "                  the halal status is to recognize if the product is halal food or not, \n",
    "                  and the healthy status is to recognize if the product is healthy if it contains a red triangle shape based on Singapore standard.\n",
    "                  Note that you can only choose one from the given options in the bracket for each label, even if you are not sure. \n",
    "                  also there is no need to add an extra note to your answer.\n",
    "                    product type (\n",
    "                        Babyfood\n",
    "                        BabyMilkPowder\n",
    "                        BeehoonVermicelli\n",
    "                        BiscuitsCrackersCookies\n",
    "                        Book\n",
    "                        BreakfastCereals\n",
    "                        CannedBakedBeans\n",
    "                        CannedBeefOtherMeats\n",
    "                        CannedBraisedPeanuts\n",
    "                        CannedChicken\n",
    "                        CannedFruits\n",
    "                        CannedMushrooms\n",
    "                        CannedPacketCreamersSweet\n",
    "                        CannedPickles\n",
    "                        CannedPorkLunchronMeat\n",
    "                        CannedSardinesMackerel\n",
    "                        CannedSoup\n",
    "                        CannedTunaDace\n",
    "                        CannedVegetarianFood\n",
    "                        ChocolateMaltPowder\n",
    "                        ChocolateSpread\n",
    "                        CoffeePowder\n",
    "                        CoffeeTeaDrink\n",
    "                        CookingCreamMilk\n",
    "                        CookingPastePowder\n",
    "                        CornChip\n",
    "                        DarkSoySauce\n",
    "                        DriedBeans\n",
    "                        DriedFruits\n",
    "                        DriedMeatSeafood\n",
    "                        DriedVegetables\n",
    "                        FlavoredMilkDrink\n",
    "                        Flour\n",
    "                        FruitJuiceDrink\n",
    "                        HerbsSpices\n",
    "                        InstantMeals\n",
    "                        InstantNoodlesMultipack\n",
    "                        InstantNoodlesSingle\n",
    "                        Jam\n",
    "                        Kaya\n",
    "                        KetchupChilliSauce\n",
    "                        LightSoySauce\n",
    "                        MaternalMilkPowder\n",
    "                        MilkDrink\n",
    "                        MilkPowder\n",
    "                        Nuts\n",
    "                        Oil\n",
    "                        OtherBakingNeeds\n",
    "                        OtherCannedBeansPeasNuts\n",
    "                        OtherCannedSeafood\n",
    "                        OtherCannedVegetables\n",
    "                        OtherDriedFood\n",
    "                        OtherHotBeveragesPowder\n",
    "                        OtherNoodles\n",
    "                        OtherSauceDressing\n",
    "                        OtherSpreads\n",
    "                        Pasta\n",
    "                        PastaSauce\n",
    "                        PeanutButter\n",
    "                        Potatochips\n",
    "                        PotatoSticks\n",
    "                        RiceBrownOthers\n",
    "                        RiceWhite\n",
    "                        RolledOatsInstantOatmeal\n",
    "                        Salt\n",
    "                        SoftDrinksOtherReadyToDrink\n",
    "                        SoupStock\n",
    "                        Sugar\n",
    "                        SweetsChocolatesOthers\n",
    "                        TeaPowderLeaves\n",
    "                        WetWiper\n",
    "                    ),\n",
    "                    weight ('400-499g', '700-799g', '500-599g', '200-299g', '100-199g',\n",
    "                       '1-99g', '300-399g', '600-699g', '800-899g', '1000-1999g',\n",
    "                       '900-999g', '3000-3999g'\n",
    "                    ),\n",
    "                    halal status ('NonHalal', 'Halal'),\n",
    "                    healthy status ('NonHealthy', 'Healthy'),\n",
    "                    \n",
    "                    Also, provide the following assessment\n",
    "                    image reflection (High, Medium, Low)\n",
    "                    image clarity (High, Medium, Low)\n",
    "                    prediction confidence for the product type (High, Medium, Low)\n",
    "                    prediction confidence for the weight (High, Medium, Low)\n",
    "                    prediction confidence for the halal status (High, Medium, Low)\n",
    "                    prediction confidence for the healthy status (High, Medium, Low)\n",
    "\n",
    "                    format your answer in json format so that it could be easily converted to dataframe\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ],\n",
    "          \"max_tokens\": 300\n",
    "        }\n",
    "\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "content = response.json()['choices'][0]['message']['content']\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "# Extracting the message content (the table with predictions)\n",
    "content = response.json()['choices'][0]['message']['content']\n",
    "\n",
    "# Parsing the content to extract prediction values\n",
    "# Splitting the content string into lines and then parsing each line\n",
    "lines = content.strip().split('\\n')[2:]  # Skipping the header\n",
    "\n",
    "# Parsing the strings to extract the relevant information\n",
    "data_parsed = [line.replace('\"', '').strip() for line in lines if ':' in line]\n",
    "data_dict = {item.split(\":\")[0].strip(): item.split(\":\")[1].strip().strip(',') for item in data_parsed}\n",
    "\n",
    "# Converting the dictionary into a DataFrame\n",
    "df_from_strings = pd.DataFrame([data_dict])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877338fa-2177-4798-92b8-0a83c99a5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92493a95-3b5f-4cd7-ab51-b0b1e7855ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['product_type', 'weight', 'halal', 'health', 'image_reflection', \n",
    "                  'image_clarity', 'product_type_confidence', 'weight_confidence',\n",
    "                  'halal_confidence', 'health_confidence']\n",
    "parsed_data = df_from_strings\n",
    "# Normalizing column names for consistency across all DataFrames\n",
    "normalized_dfs = []\n",
    "tmp_valid_paths = []\n",
    "\n",
    "\n",
    "df = df_from_strings\n",
    "if df.shape[1] != 0:\n",
    "    if df.shape[1] != 10:\n",
    "        print(\"--\\n\")\n",
    "        print(i)\n",
    "        print(df.shape[1])\n",
    "        if df.columns[-1]==\"Note\":\n",
    "            df.drop(columns=['Note'], inplace=True)\n",
    "            df.columns = col_names\n",
    "            normalized_dfs.append(df)\n",
    "            tmp_valid_paths.append(tmp_paths[i])\n",
    "        elif df.shape[1] == 11:\n",
    "            df = df.iloc[:,:-1]\n",
    "            df.columns = col_names\n",
    "    else:\n",
    "        # Renaming columns to have consistent names across all DataFrames\n",
    "        df.columns = col_names\n",
    "\n",
    "# Combining all DataFrames into a single DataFrame\n",
    "gpt_pred_df = df\n",
    "\n",
    "cols = gpt_pred_df.columns.tolist()\n",
    "gpt_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca1549-c8b2-40b7-a2e8-17b8e22be1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pred_label = gpt_pred_df['product_type'] + \"_\" + gpt_pred_df[\"weight\"] + \"_\" + gpt_pred_df[\"halal\"] + \"_\" + gpt_pred_df[\"health\"]\n",
    "gpt_pred_label.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce4f59-ed9b-4478-99e2-e231878c86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcbc762-bad3-4f2d-b060-3aa843720917",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_pred_confidence = \"Product_\" + gpt_pred_df['product_type_confidence'] + \"-Weight_\" + gpt_pred_df['weight_confidence'] + \"-Halal_\" + gpt_pred_df['halal_confidence'] + \"-Health_\" + gpt_pred_df['health_confidence']\n",
    "gpt_pred_confidence.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00bd794-b7ea-4632-afe2-5926476007c6",
   "metadata": {},
   "source": [
    "## Call R script to perform scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40cd3f-f570-4aeb-8fd1-6f82e47c1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the R_HOME environment variable to the R home directory used by RStudio\n",
    "os.environ['R_HOME'] = '/Library/Frameworks/R.framework/Resources'\n",
    "os.environ['PATH'] = '/Library/Frameworks/R.framework/Resources/bin' + os.pathsep + os.environ['PATH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea30d459-5a9f-41e2-b9d1-8cf298786ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'A': [1, 2, 3],\n",
    "#     'B': [4, 5, 6]\n",
    "# })\n",
    "\n",
    "# Serialize DataFrame to JSON\n",
    "# df_json = df.to_json()\n",
    "\n",
    "# Pass the JSON directly as an argument (ensure it's not too large)\n",
    "# input1 = df_json\n",
    "input1 = big_model_input.to_json(orient='records')\n",
    "input2 = gpt_pred_df.to_json(orient='records')\n",
    "\n",
    "# The command (ensure correct paths)\n",
    "command = ['Rscript', 'bayes_model_real_time.R', input1, input2]\n",
    "\n",
    "# Run the command and capture the output\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Check if the command was executed successfully\n",
    "if result.returncode == 0:\n",
    "    # Print stdout for debug messages and output\n",
    "    # print(\"STDOUT from R:\\n\", result.stdout)\n",
    "    # Parse the JSON output from the R script if needed\n",
    "    try:\n",
    "        output = json.loads(result.stdout)\n",
    "        print(\"Output from R:\", output)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON output.\")\n",
    "else:\n",
    "    # Print stderr for errors\n",
    "    print(\"Error running R script:\\n\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acc77a-f374-4e26-9e4b-8f62ab82e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['pred'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b0501-32e2-418f-8dcf-8bfc4ff605c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06ac90-b30f-4f4e-acd9-4dfc7c66b821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0a704-15c9-4493-857e-ed524bb6068c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
